{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1551TextsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Test and Train texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def get_claims_texts(fname):\n",
    "    with open(fname) as f:\n",
    "        all_claims_text = f.read()\n",
    "    return re.split(\"\\d{7}\", all_claims_text)\n",
    "\n",
    "claims_texts = {}\n",
    "for claim_fname in glob.glob(\"1551/*.txt\"):\n",
    "    cat_name = claim_fname[5:-4]\n",
    "    claims_texts[cat_name] = []\n",
    "    for claim_text in get_claims_texts(claim_fname):\n",
    "        claims_texts[cat_name].append(claim_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langid\n",
    "\n",
    "is_uk = lambda text: langid.classify(text)[0] == 'uk'\n",
    "\n",
    "for cat in claims_texts:\n",
    "    prev_len = len(claims_texts[cat])\n",
    "    claims_texts[cat] = [text for text in claims_texts[cat] if is_uk(text)]\n",
    "    #print(f'{cat}: {len(claims_texts[cat])}/{prev_len}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split to test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2cat = list(claims_texts.keys())\n",
    "cat2y = lambda cat : y2cat.index(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Equal classes distributions in test and train sets\n",
    "#Texts are sorted by classes\n",
    "\n",
    "import random\n",
    "\n",
    "TRAIN_SIZE = 0.7\n",
    "\n",
    "X_train_texts, Y_train = [], []\n",
    "X_test_texts, Y_test = [], []\n",
    "for cat in claims_texts:\n",
    "    y = cat2y(cat)\n",
    "    random.shuffle(claims_texts[cat])\n",
    "    train_size = int(len(claims_texts[cat]) * TRAIN_SIZE)\n",
    "    for i in range(train_size):\n",
    "        X_train_texts.append(claims_texts[cat][i])\n",
    "        Y_train.append(y)\n",
    "    for i in range(train_size, len(claims_texts[cat])):\n",
    "        X_test_texts.append(claims_texts[cat][i])\n",
    "        Y_test.append(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sasha/anaconda3/lib/python3.7/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "uk_vectors_file = 'news.lowercased.tokenized.word2vec.300d'\n",
    "uk_vectors = KeyedVectors.load_word2vec_format(uk_vectors_file, binary=False)\n",
    "\n",
    "assert len(uk_vectors[\"слово\"]) == 300\n",
    "assert uk_vectors.distance(\"слово\", \"слова\") < uk_vectors.distance(\"слово\", \"сова\")\n",
    "assert uk_vectors.similar_by_vector(uk_vectors[\"король\"] - uk_vectors[\"чоловік\"] + uk_vectors[\"жінка\"])[0][0] == 'королева'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_vector(text):\n",
    "    to_vec = lambda word : uk_vectors[word] if word in uk_vectors else np.zeros(300)\n",
    "    vectors = [to_vec(word_str) for word_str in re.split('\\W+', text)]\n",
    "    return np.sum(np.array(vectors), axis=0)\n",
    "\n",
    "X_train = [baseline_vector(text) for text in X_train_texts]\n",
    "X_test = [baseline_vector(text) for text in X_test_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
       "           metric_params=None, n_jobs=10, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import scipy\n",
    "\n",
    "def cos_dist(x, y):\n",
    "    return  scipy.spatial.distance.cosine(x, y)\n",
    "\n",
    "#knn = KNeighborsClassifier(metric=cos_dist) # worked > 1 hour\n",
    "knn = KNeighborsClassifier(metric='euclidean', n_jobs=10)\n",
    "knn.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "weighted avg       0.32      0.32      0.30     18886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sasha/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "Y_predict = knn.predict(X_test)\n",
    "\n",
    "def print_short_report(Y_test, Y_predict):\n",
    "    report_lines = classification_report(Y_test, Y_predict).split('\\n')\n",
    "    print(report_lines[0])\n",
    "    print(report_lines[-2])\n",
    "\n",
    "print_short_report(Y_test, Y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve text preprocessing\n",
    "* regexp -> tokenize_uk\n",
    "* lemmatization \n",
    "* only alphabetical words\n",
    "* stopwords\n",
    "\n",
    "### Improve vector generatoin\n",
    "* sum(vetors) -> avg(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "import tokenize_uk\n",
    "import pymorphy2\n",
    "import re\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer(lang='uk')\n",
    "stop_words = get_stop_words('ukrainian')\n",
    "\n",
    "def text2norm_words(text):\n",
    "    words = tokenize_uk.tokenize_uk.tokenize_words(text)\n",
    "\n",
    "    # f1: 0.3 -> 0.36\n",
    "    words = [w for w in words if len(w) > 3]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    words = [w.lower() for w in words]\n",
    "\n",
    "    # f1: 0.36 -> 0.39\n",
    "    words = [w for w in words if w not in stop_words]\n",
    "    words = [morph.parse(word)[0].normal_form for word in words]\n",
    "    #TODO: filter by POS\n",
    "\n",
    "    #words = list(set(words)) -> f1 -= 0.04\n",
    "    if not words:\n",
    "        words = ['']\n",
    "    return words\n",
    "\n",
    "def normalized_text_vector(text):\n",
    "    to_vec = lambda word : uk_vectors[word] if word in uk_vectors else np.zeros(300)\n",
    "    vectors = [to_vec(word) for word in text2norm_words(text)]\n",
    "    return np.sum(np.array(vectors), axis=0) / len(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [normalized_text_vector(text) for text in X_train_texts]\n",
    "X_test = [normalized_text_vector(text) for text in X_test_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
       "           metric_params=None, n_jobs=10, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(metric='euclidean', n_jobs=10)\n",
    "knn.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "weighted avg       0.42      0.39      0.39     18886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sasha/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "Y_predict = knn.predict(X_test)\n",
    "print_short_report(Y_test, Y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve classifier\n",
    "* knn -> logreg\n",
    "* TODO: tune params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sasha/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/sasha/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "weighted avg       0.55      0.56      0.54     18886\n"
     ]
    }
   ],
   "source": [
    "Y_predict = clf.predict(X_test)\n",
    "print_short_report(Y_test, Y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drafts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doc2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_doc2vec_format(X, Y):\n",
    "    res = []\n",
    "    for i in range(len(Y)):\n",
    "        res.append(gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(X[i]), [Y[i]]))\n",
    "        #res.append(gensim.models.doc2vec.TaggedDocument(text2norm_words(X[i]), [Y[i]]))\n",
    "    return res\n",
    "\n",
    "train_set = to_doc2vec_format(X_train_texts, Y_train)\n",
    "\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=10, min_count=2, epochs=40)\n",
    "model.build_vocab(train_set)\n",
    "model.train(train_set, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "doc2vec = lambda text : model.infer_vector(gensim.utils.simple_preprocess(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
       "           metric_params=None, n_jobs=10, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = [doc2vec(text) for text in X_train_texts]\n",
    "X_test  = [doc2vec(text) for text in X_test_texts]\n",
    "\n",
    "knn = KNeighborsClassifier(metric='euclidean', n_jobs=10)\n",
    "knn.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "weighted avg       0.07      0.05      0.05     18886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sasha/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "Y_predict = knn.predict(X_test)\n",
    "print_short_report(Y_test, Y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
