{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tokenize_uk\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = '1551'\n",
    "files = [entry.path\n",
    "         for entry in os.scandir(dir_name)\n",
    "         if entry.is_file()\n",
    "         and entry.name.endswith('.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188 / 188"
     ]
    }
   ],
   "source": [
    "category_names, messages, messages_category = [], [], []\n",
    "\n",
    "for i, f in enumerate(files):\n",
    "    print(f'\\r{i+1} / {len(files)}', end='')\n",
    "    category_names.append(f[len(dir_name)+1:-4].replace('-', ' '))\n",
    "    \n",
    "    with open(f) as fr:\n",
    "        lines = fr.readlines()\n",
    "    lines = ''.join(lines).split('\\n'*3)\n",
    "    \n",
    "    for m in lines:\n",
    "        try:\n",
    "            id_, text_ = m.split('\\n', 1)\n",
    "            if detect(text_) == 'uk':\n",
    "                messages.append(text_)\n",
    "                messages_category.append(i)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split data: training / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(messages, messages_category, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "with bz2.open('news.lowercased.tokenized.word2vec.300d.bz2', 'rt') as fr:\n",
    "    uk_vectors = KeyedVectors.load_word2vec_format(fr, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute vectors for a given list of texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average(vectors, weights):\n",
    "    result = weights @ vectors / np.sum(weights, axis=1, keepdims=True)\n",
    "    return result # / np.linalg.norm(result) # vector normalization doesn't improve results much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_vectors(doc, tokenizer=tokenize_uk.tokenize_words, vectorizer=CountVectorizer):\n",
    "    \"\"\"\n",
    "    Returns a list of vectors corresponding to a list of texts.\n",
    "    Vectors are weighted averages of vectors of words comprising the texts.\n",
    "    \"\"\"\n",
    "    v = vectorizer(tokenizer=tokenizer)\n",
    "    words_count = v.fit_transform( doc ).toarray()\n",
    "    vocabulary  = v.get_feature_names()\n",
    "    \n",
    "    vocabulary_vectors = [uk_vectors.get_vector(word)\n",
    "                          if word in uk_vectors\n",
    "                          else np.zeros(300)\n",
    "                          for word in vocabulary ]\n",
    "    \n",
    "    vectors_found = [1 if word in uk_vectors else 10**-10 for word in vocabulary]  # 10^-10 is used to avoid div/0\n",
    "    \n",
    "    return weighted_average(vocabulary_vectors, words_count * vectors_found )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_report(y_test, y_predict):\n",
    "    report = classification_report(y_test, y_predict).split('\\n')\n",
    "    print(report[0])\n",
    "    print(report[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Baseine - KNN, no TF-IDF, no lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_KNN(tokenizer=tokenize_uk.tokenize_words, vectorizer=CountVectorizer):\n",
    "    train_vectors = messages_vectors( x_train, tokenizer, vectorizer )\n",
    "    test_vectors = messages_vectors( x_test, tokenizer, vectorizer )\n",
    "    \n",
    "    knn = KNeighborsClassifier(metric='euclidean')\n",
    "    knn.fit(train_vectors, y_train)\n",
    "    \n",
    "    y_predict = knn.predict(test_vectors)\n",
    "    \n",
    "    print_report(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "weighted avg       0.37      0.34      0.33     18739\n"
     ]
    }
   ],
   "source": [
    "evaluate_KNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Class average vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = messages_vectors( x_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vectors = messages_vectors( x_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = ([[1 if y_train[j] == category else 0\n",
    "             for j in range(len(x_train))]\n",
    "            for category in range(len(category_names))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_vectors = weighted_average(train_vectors, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar(vector) -> int:\n",
    "    similarity = KeyedVectors.cosine_similarities(vector, category_vectors)\n",
    "    if sum(vector) == 0:\n",
    "        # can happen when none of the words from the message are found in a vocabulary\n",
    "        # for exemple \"Відсутне опаленненя\"\n",
    "        return 158 # most common category - \"Відсутність-ГВП\"\n",
    "    else:\n",
    "        return np.where(similarity == np.nanmax(similarity))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = [ find_most_similar( v ) for v in test_vectors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "weighted avg       0.44      0.21      0.24     18739\n"
     ]
    }
   ],
   "source": [
    "print_report(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observations:\n",
    "\n",
    "1. Aggregation of message vectors to represent a whole class of messages results in higher precisioin but lower recall as compared to kNN with vectors for individual messages.\n",
    "2. We can clearly see a corelation between precision and a number of class instances in our training samples:\n",
    "\n",
    "<img src='Precision.png' width=350> \n",
    "\n",
    "Highest precision of 93% is seen for the category with highest number of training samples.\n",
    "\n",
    "3. Alternatevely, high precision is also seen in categories with some very distinctive features. For example, category \\#178 \"Заміна та експлуатація поштових скриньок\" having only 142 training samples has precision of 90% - it contains a very distinctive \"поштових скриньок\" phrase in every message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Apply TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "weighted avg       0.40      0.36      0.35     18739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aromanov/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "evaluate_KNN( vectorizer=TfidfVectorizer )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observation\n",
    "TF-IDF resulted in a bit higher precison and recall than the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Apply lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer(lang='uk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmed_tokenizer(doc):\n",
    "    return [morph.parse(w)[0].normal_form for w in tokenize_uk.tokenize_words(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "weighted avg       0.39      0.36      0.36     18739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aromanov/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "evaluate_KNN(vectorizer=TfidfVectorizer, tokenizer=stemmed_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observations\n",
    "1. Lemmatization has slightly improved recall.\n",
    "2. Most messages contain numerous typos. We supposedly may increase precision by correcting errors in spelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Logistic regression on message vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = messages_vectors( x_train, vectorizer=TfidfVectorizer, tokenizer=stemmed_tokenizer )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vectors = messages_vectors( x_test, vectorizer=TfidfVectorizer, tokenizer=stemmed_tokenizer )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(train_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = logreg.predict(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "weighted avg       0.56      0.57      0.56     18739\n"
     ]
    }
   ],
   "source": [
    "print_report(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observation\n",
    "\n",
    "1. Linear classifiers seem to be able to pick up some extra information contained in vectors which improves substantially precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
