{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics!\n",
    "\n",
    "Для свого курсового проекту визначте остаточні метрики і напишіть програму, яка їх реалізує. Покажіть приклад роботи програми на іграшкових даних (до 10 прикладів реальних чи штучних даних).\n",
    "\n",
    "\n",
    "## Поділ на склади\n",
    "\n",
    "* якщо між голосними звуками є один приголосний, то він належить до наступного складу: о-сінь, про-синь, Си-дір;\n",
    "* якщо між голосними є кілька приголосних, то й, в, р, л, м, н, які стоять після голосного, належать до попереднього складу, а звуки, що йдуть після них, — до наступного: гай-ка, Вол-га, мір-ка;\n",
    "* якщо другим приголосним виступають й, р, л, то разом з попереднім приголосним вони відходять до наступного складу: мі-льйон, лю-блю, Дми-тро;\n",
    "* якщо між голосними є кілька приголосних, то після наголосу один з них відходить до попереднього, а решта — до наступного складу: мас-ло, eip-ний, рів-ний;\n",
    "* якщо після ненаголошеного складу стоїть кілька приголосних, то всі вони, крім й, в, л, відходять до наступного: се-стра, пе-ре-рва-ти, про-сте-жи-ти."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllable_break(s):\n",
    "    \n",
    "    s = s.lower()\n",
    "    \n",
    "    alphabet = set('абвгґдеєжзиіїйклмнопрстуфхцчшщьюя’')\n",
    "    vow = set('аеиіеоуяєїєю')\n",
    "    cons = set('бвгґджзйклмнпрстфхцчшщь')\n",
    "    \n",
    "    group1 = set(\"йврлмн\")\n",
    "    group2 = set('йрл')\n",
    "    group3 = cons.symmetric_difference(set('йвл'))    \n",
    "    \n",
    "    breaks = []\n",
    "    n = len(s)\n",
    "    \n",
    "    for i in range(0, n):\n",
    "        \n",
    "        \"\"\"якщо між голосними звуками є один приголосний, то \n",
    "        він належить до наступного складу: о-сінь, про-синь, Си-дір;\"\"\"\n",
    "        \n",
    "        if i < n-2 and s[i] in vow and s[i+1] in cons and s[i+2] in vow:\n",
    "            print('{0}-{1}{2}'.format(s[i], s[i+1], s[i+2]))\n",
    "            breaks.append(i+1)\n",
    "            \n",
    "        \"\"\"якщо між голосними є кілька приголосних, то й, в, р, л, м, н, \n",
    "        які стоять після голосного, належать до попереднього складу, а \n",
    "        звуки, що йдуть після них, — до наступного: гай-ка, Вол-га, мір-ка;\"\"\"\n",
    "        \n",
    "        if i < n-2 and s[i] in vow and s[i+1] in group1 and s[i+2] in cons:\n",
    "            breaks.append(i+1)\n",
    "            \n",
    "        \"\"\"якщо другим приголосним виступають й, р, л, то разом з попереднім \n",
    "        приголосним вони відходять до наступного складу: мі-льйон, лю-блю, \n",
    "        Дми-тро;\"\"\"\n",
    "        \n",
    "        if i < n-1 and s[i] in cons and s[i+1] in group2:\n",
    "            breaks.append(i-1)\n",
    "            \n",
    "        \"\"\"якщо між голосними є кілька приголосних, то після наголосу один \n",
    "        з них відходить до попереднього, а решта — до наступного складу: \n",
    "        мас-ло, вip-ний, рів-ний;\"\"\"\n",
    "        \n",
    "        \"\"\"якщо після ненаголошеного складу стоїть кілька приголосних, то всі \n",
    "        вони, крім й, в, л, відходять до наступного: се-стра, пе-ре-рва-ти, \n",
    "        про-сте-жи-ти.\"\"\"\n",
    "        \n",
    "    \n",
    "    print(breaks)\n",
    "    \n",
    "\n",
    "    \n",
    "syllable_break('люблю')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визначення наголосу\n",
    "\n",
    "Щоб визначити, чи відповідає той чи інший рядок поетичному метру, а також щоб правильно розбивати слова на склади, нам необхідно реалізувати програму, яка буде коректно виставляти наголоси в українських словах.\n",
    "\n",
    "Огляд джерел показує, що детерміністичних правил наголошення в українській мові немає. Натомість, існує декілька загальних вказівок, а також словники, де в словах вказується наголос (зокрема, [орфоепічний](http://irbis-nbuv.gov.ua/cgi-bin/ua/elib.exe?Z21ID=&I21DBN=UKRLIB&P21DBN=UKRLIB&S21STN=1&S21REF=10&S21FMT=online_book&C21COM=S&S21CNR=20&S21P01=0&S21P02=0&S21P03=FF=&S21STR=ukr0001233). Також наголоси наводяться для частини слів у [СУМі](http://sum.in.ua)).\n",
    "\n",
    "Отже, пропонуємо отримати вихідні дані з СУМу, після чого на цих даних спробувати навчити ML-модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Set changed size during iteration",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-e113257477f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m \u001b[0mscrape_indices_boot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-45-e113257477f0>\u001b[0m in \u001b[0;36mscrape_indices_boot\u001b[0;34m()\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_visited\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0mlinks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_url\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m                 \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m                 \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_as_visited\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-e113257477f0>\u001b[0m in \u001b[0;36mupdate_indices\u001b[0;34m(indices, entries, links)\u001b[0m\n\u001b[1;32m    226\u001b[0m     \"\"\"\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munvisited\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;31m# Caches are added to avoid changing set size during iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Set changed size during iteration"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "import json\n",
    "\n",
    "from pprint import pprint as pp\n",
    "\n",
    "import trident\n",
    "\n",
    "class BannedError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def dump(entries, target_file='dump.txt'):\n",
    "    \"\"\"Dump list of entries (words, urls) into a newline-delimited text file\"\"\"\n",
    "    \n",
    "    try:\n",
    "        with open(target_file, 'w+') as f:\n",
    "            for entry in entries:\n",
    "                f.write(entry + '\\n')\n",
    "    except:\n",
    "        return -1\n",
    "    \n",
    "    return 0\n",
    "\n",
    "\n",
    "# Scraper code\n",
    "def run_request(url):\n",
    "    \"\"\"Fetch & soupify url. Use sparingly\"\"\"\n",
    "    \n",
    "    # This will mask us as a legit user and help avoid 424\n",
    "    headers = {\n",
    "        'User-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/12.0.3 Safari/605.1.15',\n",
    "        'Referer': 'https://sum.in.ua/',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "        'Accept-Encoding': 'br, gzip, deflate',\n",
    "        'Host': 'sum.in.ua',\n",
    "        'Connection': 'keep-alive'\n",
    "    }\n",
    "    \n",
    "    ban_line = 'Вибачте, доступ до сайту тимчасово закрито.'\n",
    "    \n",
    "    session = trident.init()\n",
    "    # session = requests.session(headers=headers)\n",
    "    \n",
    "    # Run request\n",
    "    r = session.get(url, headers=headers)\n",
    "    soup = bs(r.text, 'html.parser')\n",
    "    \n",
    "    if ban_line in soup.text:\n",
    "        raise BannedError(\"You have been temporarily banned. Wait until the ban is removed or change your IP address.\")\n",
    "        trident.shift()\n",
    "        \n",
    "    # Return a BS4 object\n",
    "    return soup\n",
    "\n",
    "\n",
    "def get_word(url):\n",
    "    \"\"\"Scrape article url to obtain word stress information\n",
    "    from sum.in.ua\n",
    "    \n",
    "    :rtype word - string with stressed vowel in uppercase e.g. 'safAri'\n",
    "    or -1 if no stress information was obtained from target article\"\"\"\n",
    "    \n",
    "    soup = run_request(url)\n",
    "    word = ''\n",
    "    \n",
    "    # Some articles don't have stress info, so this \n",
    "    # flag will indicate if stress info was scraped\n",
    "    article_contains_stress = False\n",
    "    \n",
    "    word_tag = soup.find_all(class_='title')[0]\n",
    "\n",
    "    for child in word_tag.children:\n",
    "        if type(child) == bs4.element.NavigableString:\n",
    "            word += child.lower()\n",
    "        elif type(child) == bs4.element.Tag:\n",
    "            class_ = child.get('class')[0]\n",
    "            if class_ == 'stressed':\n",
    "                word += child.text.upper()\n",
    "                article_contains_stress = True\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    # If no stress info was obtained from target \n",
    "    # article, return an error value\n",
    "    if article_contains_stress:\n",
    "        return word\n",
    "    else: \n",
    "        return -1\n",
    "\n",
    "def get_index(url):\n",
    "    \"\"\"Scrape letter index url to obtain list of words to scrape\"\"\"\n",
    "\n",
    "    soup = run_request(url)    \n",
    "    link_list = []\n",
    "    \n",
    "    # print(soup)\n",
    "        \n",
    "    for li in soup.find_all('li'):\n",
    "        for a in li.children:\n",
    "            href = a.get('href')\n",
    "            link_list.append(href)\n",
    "    \n",
    "    return link_list\n",
    "    \n",
    "\n",
    "# Rules to idetify types of links encountered\n",
    "def is_index(s):\n",
    "    if len(re.findall(r'\\/vkazivnyk\\/', s)) > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def is_entry(s):\n",
    "    if len(re.findall(r'\\/s\\/', s)) > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def scrape_indices_boot():\n",
    "    base_url = 'http://sum.in.ua/vkazivnyk/'\n",
    "    root_url = 'http://sum.in.ua'\n",
    "    \n",
    "    indices = PersistentIndex(visited_name='indices_visited.txt', \n",
    "                              unvisited_name='indices_unvisited.txt')\n",
    "    \n",
    "    entries = PersistentIndex(visited_name='entries_visited.txt',\n",
    "                              unvisited_name='entries_unvisited.txt')\n",
    "    \n",
    "    links = get_index(base_url)\n",
    "    \n",
    "    indices, entries = update_indices(indices, entries, links)\n",
    "    \n",
    "    while len(indices.unvisited) > 0:\n",
    "        for item in indices.unvisited:\n",
    "            if not indices.is_visited(item):\n",
    "                links = get_index(root_url + item)\n",
    "                indices, entries = update_indices(indices, entries, links)\n",
    "                indices.mark_as_visited(item)\n",
    "            \n",
    "    # print(indices.unvisited)        \n",
    "    indices.save_state()\n",
    "    entries.save_state()\n",
    "\n",
    "def update_indices(indices, entries, links):\n",
    "    \"\"\"\n",
    "    go into each index\n",
    "    get subindex\n",
    "    compare it with visited and unvisited\n",
    "        add new unvisited\n",
    "    \n",
    "    return indices, entries, links\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    for item in indices.unvisited:\n",
    "        \n",
    "        # Caches are added to avoid changing set size during iteration\n",
    "        indices_cache = set()\n",
    "        entries_cache = set()\n",
    "        \n",
    "        for link in links:\n",
    "            if is_index(link) and not indices.is_visited(link):\n",
    "                indices_cache.add(link)\n",
    "            if is_entry(link) and not entries.is_visited(link):\n",
    "                entries_cache.add(link)\n",
    "        \n",
    "        indices.update_unvisited(indices_cache)\n",
    "        entries.update_unvisited(entries_cache)\n",
    "        \n",
    "    return indices, entries\n",
    "    \n",
    "scrape_indices_boot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i.is_unvisited('index1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://sum.in.ua/vkazivnyk/ab'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "words = []\n",
    "urls = get_index(url)\n",
    "\n",
    "for word_url in urls:\n",
    "    words.append(get_word('http://sum.in.ua/'+word_url))\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "session = requests.session()\n",
    "session.proxies = {\n",
    "    'http' : 'socks5h://localhost:9050',\n",
    "    'https' : 'socks5h://localhost:9050'\n",
    "}\n",
    "\n",
    "r = session.get('http://httpbin.org/ip')\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"origin\": \"38.117.96.154, 38.117.96.154\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import trident\n",
    "\n",
    "session = trident.init()\n",
    "\n",
    "r = session.get('http://httpbin.org/ip')\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trident.shift(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"origin\": \"38.117.96.154, 38.117.96.154\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = session.get('http://httpbin.org/ip')\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4/9/19, 12:22:16.669 [NOTICE] Bootstrapped 10%: Finishing handshake with directory server \n",
    "4/9/19, 12:22:16.787 [NOTICE] Bootstrapped 15%: Establishing an encrypted directory connection \n",
    "4/9/19, 12:22:16.830 [NOTICE] Bootstrapped 20%: Asking for networkstatus consensus \n",
    "4/9/19, 12:22:16.876 [NOTICE] Bootstrapped 25%: Loading networkstatus consensus \n",
    "4/9/19, 12:22:17.735 [NOTICE] Bootstrapped 45%: Asking for relay descriptors \n",
    "4/9/19, 12:22:17.736 [NOTICE] I learned some more directory information, but not enough to build a circuit: We need more microdescriptors: we have 3904/6640, and can only build 30% of likely paths. (We have 67% of guards bw, 64% of midpoint bw, and 69% of exit bw = 30% of path bw.) \n",
    "4/9/19, 12:22:18.372 [NOTICE] Bootstrapped 64%: Loading relay descriptors \n",
    "4/9/19, 12:22:18.372 [NOTICE] Bootstrapped 70%: Loading relay descriptors \n",
    "4/9/19, 12:22:19.369 [NOTICE] Bootstrapped 76%: Loading relay descriptors \n",
    "4/9/19, 12:22:19.370 [NOTICE] Bootstrapped 80%: Connecting to the Tor network \n",
    "4/9/19, 12:22:19.371 [NOTICE] Bootstrapped 85%: Finishing handshake with first hop \n",
    "4/9/19, 12:22:19.372 [NOTICE] Bootstrapped 90%: Establishing a Tor circuit \n",
    "4/9/19, 12:22:19.373 [NOTICE] Bootstrapped 100%: Done \n",
    "4/9/19, 12:22:20.576 [NOTICE] New control connection opened from 127.0.0.1. \n",
    "4/9/19, 12:22:20.826 [NOTICE] New control connection opened from 127.0.0.1. \n",
    "4/9/19, 12:37:06.263 [NOTICE] New control connection opened from 127.0.0.1. \n",
    "4/9/19, 12:40:32.779 [NOTICE] New control connection opened from 127.0.0.1. \n",
    "4/9/19, 12:43:53.387 [NOTICE] New control connection opened from 127.0.0.1. \n",
    "4/9/19, 12:44:09.960 [NOTICE] New control connection opened from 127.0.0.1. \n",
    "4/9/19, 12:53:17.445 [NOTICE] New control connection opened from 127.0.0.1. \n",
    "4/9/19, 12:53:23.768 [NOTICE] New control connection opened from 127.0.0.1. \n",
    "4/9/19, 12:53:23.772 [NOTICE] Rate limiting NEWNYM request: delaying by 4 second(s) \n",
    "4/9/19, 12:53:30.228 [NOTICE] New control connection opened from 127.0.0.1. \n",
    "4/9/19, 12:53:30.236 [NOTICE] Rate limiting NEWNYM request: delaying by 7 second(s) \n",
    "4/9/19, 12:55:01.799 [NOTICE] New control connection opened from 127.0.0.1. \n",
    "4/9/19, 12:55:11.163 [NOTICE] New control connection opened from 127.0.0.1. \n",
    "4/9/19, 12:55:11.163 [NOTICE] Rate limiting NEWNYM request: delaying by 1 second(s) \n",
    "4/9/19, 12:58:21.690 [NOTICE] New control connection opened from 127.0.0.1. \n",
    "4/9/19, 13:00:10.573 [NOTICE] New control connection opened from 127.0.0.1. \n",
    "4/9/19, 13:08:42.471 [NOTICE] New control connection opened from 127.0.0.1. \n",
    "4/9/19, 13:08:53.414 [NOTICE] New control connection opened from 127.0.0.1. \n",
    "4/9/19, 13:09:52.294 [NOTICE] New control connection opened from 127.0.0.1. \n",
    "4/9/19, 13:09:59.238 [NOTICE] New control connection opened from 127.0.0.1. \n",
    "4/9/19, 13:09:59.242 [NOTICE] Rate limiting NEWNYM request: delaying by 3 second(s) \n",
    "4/9/19, 13:10:46.550 [NOTICE] New control connection opened from 127.0.0.1. \n",
    "4/9/19, 13:11:38.454 [NOTICE] New control connection opened from 127.0.0.1. \n",
    "4/9/19, 13:12:20.274 [NOTICE] New control connection opened from 127.0.0.1. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
