{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs of a wonderful website of Ukrainian poetry\n",
    "url_base = \"https://onlyart.org.ua\"\n",
    "\n",
    "# We intend to scrape three main categories:\n",
    "url_classics = \"/ukrainian-poets/\"\n",
    "url_modern = \"/modern-ukrainian-poets/\"\n",
    "url_live = \"/live/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The general strategy is to scrape all three sections (the 'classics' and \n",
    "'modern' sections look nearly identical, the 'live' section will require a slightly\n",
    "different approcah with pagination support, different author parsing, etc.)\n",
    "\n",
    "Resulting data is stored as JSON like so:\n",
    "\n",
    "{\n",
    "    poet_name: {\n",
    "        poem_title: poem_text,\n",
    "        ...\n",
    "    }\n",
    "    ...\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "import json\n",
    "\n",
    "from pprint import pprint as pp\n",
    "\n",
    "\n",
    "def run_request(url):\n",
    "    \"\"\"Fetch & soupify url. Use sparingly\"\"\"\n",
    "    \n",
    "    # This will mask us as a legit user and help avoid 424\n",
    "    headers = {\n",
    "        'User-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/12.0.3 Safari/605.1.15',\n",
    "        'Referer': 'https://onlyart.org.ua/',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "        'Accept-Encoding': 'br, gzip, deflate',\n",
    "        'Host': 'onlyart.org.ua',\n",
    "        'Connection': 'keep-alive'\n",
    "    }\n",
    "    \n",
    "    # Run request\n",
    "    r = requests.get(url, headers=headers)\n",
    "    soup = bs(r.text, 'html.parser')\n",
    "    \n",
    "    # Return a BS4 object\n",
    "    return soup\n",
    "\n",
    "\n",
    "def get_poet_urls(url):\n",
    "    \"\"\"Scraping step 1:\n",
    "    Gets urls for all poets in a section:\n",
    "    base_url -> {poet_name: poet_url}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Parse and target 'posts' section\n",
    "    soup = run_request(url)\n",
    "    posts = soup.find(id='posts')\n",
    "\n",
    "    poet_urls = {}\n",
    "\n",
    "    for post in posts:\n",
    "        if post != '\\n':\n",
    "            try:\n",
    "                link = post.a['href']\n",
    "                name = post.a.string\n",
    "            except TypeError:\n",
    "                pass\n",
    "            poet_urls[name] = link\n",
    "    \n",
    "    # Return populated dict\n",
    "    return poet_urls\n",
    "\n",
    "\n",
    "def get_poem_urls(url):\n",
    "    \"\"\"Scraping step 2:\n",
    "    Gets urls of poem pages from a poet section page:\n",
    "    poet_url -> {poem_name: poem_url}\"\"\"\n",
    "    \n",
    "    poems = {}\n",
    "\n",
    "    soup = run_request(url)    \n",
    "    entries = soup.find_all(class_='entry')\n",
    "\n",
    "    for entry in entries:\n",
    "        a_tags = entry.find_all('a')\n",
    "        for tag in a_tags:\n",
    "            link = tag['href']\n",
    "\n",
    "            # This is an ugly hack to merge all text \n",
    "            # regardless of nested tags\n",
    "            name = tag.text\n",
    "            \n",
    "            \"\"\"\n",
    "            name = ''\n",
    "            \n",
    "            contents = tag.contents\n",
    "            for item in contents:\n",
    "                item = item.string\n",
    "                name = name + item\n",
    "            \"\"\"\n",
    "\n",
    "            poems[name] = link\n",
    "    \n",
    "    return poems\n",
    "    \n",
    "\n",
    "def get_poem_text(url):\n",
    "    \"\"\"Scraping step 3:\n",
    "    Fetch and return\n",
    "    \n",
    "    TODO: Separate titles and subtitles\n",
    "    \"\"\"\n",
    "    \n",
    "    stop = {\n",
    "        '(adsbygoogle=window.adsbygoogle||[]).push({});',\n",
    "        '\\n\\n'\n",
    "    }\n",
    "    \n",
    "    # Fetch text\n",
    "    soup = run_request(url)\n",
    "    poem_text = soup.article.text\n",
    "    \n",
    "    # Remove stopwords\n",
    "    for word in stop:\n",
    "        poem_text = poem_text.replace(word, '')\n",
    "    \n",
    "    return poem_text\n",
    "    \n",
    "    \n",
    "def scrape_section(url):\n",
    "    \"\"\"Scrape entire section\n",
    "    \n",
    "    WARNING: Takes a very long time and puts strain on target server.\n",
    "    Use with caution.\"\"\"\n",
    "    \n",
    "    poets = get_poet_urls(url)\n",
    "    \n",
    "    for poet in poets.keys():\n",
    "        poems = get_poem_urls(poets[poet])\n",
    "        for poem in poems.keys():\n",
    "            \n",
    "            # A small number of poems will fail to fetch,\n",
    "            # we just ignore them for now.\n",
    "            try:\n",
    "                text = get_poem_text(poems[poem])\n",
    "                poems[poem] = text\n",
    "            except:\n",
    "                text = \"#ERROR at:\"+poems[poem]\n",
    "        \n",
    "        poets[poet] = poems\n",
    "    \n",
    "    return poets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run routine\n",
    "\n",
    "url = url_base + url_classics\n",
    "poets = scrape_section(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to a file\n",
    "\n",
    "s = json.dumps(poets)\n",
    "with open('poets.json', 'w+') as f:\n",
    "    f.write(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'збе-ре-же-н-ня'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyphen \n",
    "\n",
    "dic = pyphen.Pyphen(lang='uk_UA')\n",
    "dic.inserted('збереження')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
