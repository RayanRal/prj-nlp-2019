{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Мова як послідовність\n",
    "\n",
    "## I. Run-on Sentences\n",
    "\n",
    "### 1. Домен\n",
    "\n",
    "Цього тижня ви працюватимете над задачею виправлення помилок.\n",
    "\n",
    "Run-on речення - це речення, склеєне з двох чи більше речень без належної пунктуації. Таку помилку часто допускають механічно, коли швидко друкують текст, проте така помилка виникає і від незнання мови. Особливо часто ця помилка зустрічається в інтернет-спілкуванні.\n",
    "\n",
    "Наприклад:\n",
    "```\n",
    "Thanks for talking to me let's meet again tomorrow Bye.\n",
    "```\n",
    "\n",
    "У цьому реченні насправді три склеєні речення. Правильний варіант:\n",
    "```\n",
    "Thanks for talking to me. Let's meet again tomorrow. Bye.\n",
    "```\n",
    "\n",
    "Run-on речення важливо визначати не лише для виправлення помилок. Ця помилка впливає на якість визначення сутностей, частин мови, синтаксичних зв'язків тощо.\n",
    "\n",
    "Більше інформації та прикладів можна знайти за посиланнями:\n",
    "- http://www.bristol.ac.uk/arts/exercises/grammar/grammar_tutorial/page_37.htm\n",
    "- https://www.english-grammar-revolution.com/run-on-sentence.html\n",
    "- https://www.quickanddirtytips.com/education/grammar/what-are-run-on-sentences\n",
    "\n",
    "### 2. Класифікатор\n",
    "\n",
    "Дані:\n",
    "- Виберіть будь-який відкритий корпус та згенеруйте тренувальні дані для моделі. Тренувальними даними буде набір склеєних речень. Візьміть до уваги, що склеєних речень може бути кілька (зазвичай 2, але буває і 3-4), а перше слово наступного речення може писатися з великої чи малої літери.\n",
    "- Зберіть (чи знайдіть у відкритому доступі) базу енграмів. Візьміть до уваги, що відкриті бази енграмів зазвичай містять статистику, зібрану на реченнях, а отже вони можуть не містити енграми на межі речень.\n",
    "\n",
    "Тестування:\n",
    "- Напишіть бейзлайн та метрику для тестування якості.\n",
    "- Для тестування використайте корпус [run-on-test.json](run-on-test.json). Формат корпусу:\n",
    "```\n",
    "[\n",
    "  [\n",
    "    [\"Thanks\", false],\n",
    "    [\"for\", false],\n",
    "    [\"talking\", false],\n",
    "    [\"to\", false],\n",
    "    [\"me\", true],\n",
    "    [\"let\", false],\n",
    "    [\"'s\", false],\n",
    "    [\"meet\", false],\n",
    "    [\"again\", false],\n",
    "    [\"tomorrow\", true],\n",
    "    [\"Bye\", false],\n",
    "    [\".\", false]\n",
    "  ],\n",
    "...\n",
    "]\n",
    "```\n",
    "\n",
    "`true` позначає слово, на якому закінчується речення. Тестовий корпус містить 200 речень (~ 4700 токенів). 3% токенів мають клас `true`, а решта - `false`.\n",
    "\n",
    "Класифікатор:\n",
    "- Виділіть ознаки, які впливають на те, чи є слово на межі речень. Подумайте про правий/лівий контекст, написання слова, граматичні ознаки (чи може речення закінчитись на сполучник?), енграми (чи часто це слово і наступне йдуть поруч?), складники та залежності тощо.\n",
    "- Побудуйте класифікатор на основі логістичної регресії з використанням виділених ознак, який анотує послідовно слова у реченні на предмет закінчення речення.\n",
    "- Спробуйте покращити якість роботи класифікатора, змінюючи набір чи комбінацію ознак."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "import string\n",
    "import pickle\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "from spacy.tokens import Doc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option('display.max_colwidth', 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data\n",
    "1. Dataset of simplified text from wikipedia (for training)\n",
    "\n",
    "Знайшла датасет документів з Simple English Wikipedia, який був створений для Text simplification.<br>\n",
    "Я взяла тільки один файл зі спрощеними реченнями - simple.txt\n",
    "\n",
    "Посилання - http://www.cs.pomona.edu/~dkauchak/simplification/\n",
    "\n",
    "2. Frequent trigrams (for extra feature)\n",
    "\n",
    "Coca 3-grams for words which occur at least 10 times\n",
    "\n",
    "Посилання - https://www.wordfrequency.info/ngrams.asp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en\", disable=['tagger', 'parser', 'ner'])\n",
    "\n",
    "def get_data(file='Data/document-aligned.v2/simple.txt'):\n",
    "    with open(file) as f:\n",
    "        prev_subj = 'April'\n",
    "        prev_num = '0'\n",
    "        subject = ''\n",
    "        num = '0'\n",
    "        sentence = ''\n",
    "        paragraphs = []\n",
    "        paragraph = []\n",
    "\n",
    "        for line in f.readlines():\n",
    "            elements = line.rstrip('\\n').split('\\t')\n",
    "            subject = elements[0]\n",
    "            num = elements[1]\n",
    "            sentence = elements[2]\n",
    "            tokens = nlp(sentence)\n",
    "            \n",
    "            if subject != prev_subj and num != prev_num and paragraph:\n",
    "                paragraph[-1][1] = False\n",
    "                paragraph.append(['</p>', False])\n",
    "                \n",
    "                paragraphs.append(paragraph)\n",
    "                paragraph = []\n",
    "            \n",
    "            for i, token in enumerate(tokens[:-1]):  # dot is last character\n",
    "                if not paragraph:\n",
    "                    paragraph.append(['<p>', False])\n",
    "\n",
    "                if i == 0 and len(paragraph) > 1:  # first word of the sentence\n",
    "                    # randomly make a word lower- or upper-case\n",
    "                    random_num = random.randint(0,1)\n",
    "                    if random_num == 1:\n",
    "                        current_token =  token.text.lower()\n",
    "                    else:\n",
    "                        current_token =  token.text.title()\n",
    "                    paragraph.append([current_token, False])\n",
    "                elif i == len(tokens) - 2:\n",
    "                    paragraph.append([token.text, True])\n",
    "                else:\n",
    "                    paragraph.append([token.text, False])\n",
    "            \n",
    "            prev_subj = subject\n",
    "            prev_num = num\n",
    "        return paragraphs\n",
    "    \n",
    "# paragraphs = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<p>', 0],\n",
       " ['April', 0],\n",
       " ['is', 0],\n",
       " ['the', 0],\n",
       " ['fourth', 0],\n",
       " ['month', 0],\n",
       " ['of', 0],\n",
       " ['the', 0],\n",
       " ['year', 1],\n",
       " ['it', 0]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_to_pickle(path, item):\n",
    "    with open(path, 'ab') as file:\n",
    "        pickle.dump(item, file, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def read_from_pickle(path):\n",
    "    objects = []\n",
    "    with (open(path, \"rb\")) as openfile:\n",
    "        while True:\n",
    "            try:\n",
    "                objects.append(pickle.load(openfile))\n",
    "            except EOFError:\n",
    "                break\n",
    "    return objects\n",
    "\n",
    "\n",
    "# add_to_pickle('Data/procc_dataset.pkl', paragraphs)\n",
    "paragraphs = read_from_pickle('Data/procc_dataset.pkl')[0][0]\n",
    "paragraphs[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Frequent N-grams Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('free <p>', 564.0),\n",
       " ('free \"', 501.0),\n",
       " ('free the', 225.0),\n",
       " ('free i', 159.0),\n",
       " ('free but', 129.0),\n",
       " ('free and', 125.0),\n",
       " ('free call', 115.0),\n",
       " ('free it', 111.0),\n",
       " ('free he', 91.0),\n",
       " ('free they', 70.0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_freq_trigrams():\n",
    "    with open('Data/ngrams_alpha.txt', 'r') as f:\n",
    "        lines = f.read().split('\\n')\n",
    "        lines_list = [line.split('\\t') for line in lines]\n",
    "        df = pd.DataFrame(lines_list, columns=['freq', 'w1', 'w2', 'w3'])\n",
    "        df['freq'] = pd.to_numeric(df['freq'])\n",
    "        df['w3'] = df['w3'].map(lambda x: str(x).lower())\n",
    "        df = df[df.apply(lambda x: True if (x['w2']=='.') else False, axis=1)]\n",
    "        df['trigrams_dict'] = df.apply(lambda x: {str(x['w1']) + ' ' +str(x['w3']): x['freq']}, axis=1)\n",
    "    \n",
    "    trigrams_list = df['trigrams_dict'].tolist()\n",
    "    trigrams_dict = {key: value for item in trigrams_list for key, value in item.items()}\n",
    "\n",
    "    return trigrams_dict\n",
    "\n",
    "# df.sort_values(by='freq', ascending=False).head(10)\n",
    "# 14594 lines with dot as second word\n",
    "\n",
    "trigrams_dict  = get_freq_trigrams()\n",
    "list(trigrams_dict.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = max(trigrams_dict.values())\n",
    "b = min(trigrams_dict.values())\n",
    "\n",
    "c = 0\n",
    "d = 10\n",
    "\n",
    "def transform_num(x, a=a, b=b, c=c, d=d):\n",
    "    if x:\n",
    "        y = (x - a) * (d - c)/(b - a) + c\n",
    "        return round(y)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "transform_num(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функція ставить у відповідність числу з [10, ..., 5905] -> число в [0, ..., 10]. Таким чином маємо ознаку, яка має 11 можливих значень: 0 - крапка між даними словами вживається рідко, .., 10 - вживається часто."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepair_X_y(paragraphs):\n",
    "    X = []\n",
    "    y = []\n",
    "    for p in paragraphs:\n",
    "        X.append([word for word, _ in p])\n",
    "        y.append([boo for _, boo in p])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = prepair_X_y(paragraphs)\n",
    "\n",
    "X_train_, X_test_, y_train_, y_test_ = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "X_train_ = X_train_[:5000]\n",
    "X_test_ = X_test_[:1000]\n",
    "y_train = [token for x in y_train_[:5000] for token in x]\n",
    "y_test =[token for x in y_test_[:1000] for token in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class WordTokenizer(object):\n",
    "    \"\"\"\n",
    "    Custom Tokenizer\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab=nlp.vocab, tokenizer=None, return_doc=True):\n",
    "        self.vocab = vocab\n",
    "        self._word_tokenizer = tokenizer\n",
    "        self.return_doc = return_doc\n",
    "\n",
    "    def __call__(self, text):\n",
    "        if self._word_tokenizer:\n",
    "            words = self._word_tokenizer.tokenize(text)\n",
    "        else:\n",
    "            words = text.split(' ')\n",
    "        if self.return_doc:\n",
    "            spaces = [True] * len(words)\n",
    "            return Doc(self.vocab, words=words, spaces=spaces)\n",
    "        else:\n",
    "            return words\n",
    "\n",
    "nlp = spacy.load('en', disable=['ner'])\n",
    "nlp.tokenizer = WordTokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>dep</th>\n",
       "      <th>capital</th>\n",
       "      <th>paragraph_boundary</th>\n",
       "      <th>number</th>\n",
       "      <th>dot</th>\n",
       "      <th>is_punct</th>\n",
       "      <th>prev_lemma</th>\n",
       "      <th>prev_token_pos</th>\n",
       "      <th>...</th>\n",
       "      <th>prev_paragraph_boundary</th>\n",
       "      <th>prev_capital</th>\n",
       "      <th>next_lemma</th>\n",
       "      <th>next_token_pos</th>\n",
       "      <th>next_dep</th>\n",
       "      <th>next_paragraph_boundary</th>\n",
       "      <th>next_capital</th>\n",
       "      <th>neighbors</th>\n",
       "      <th>bigram</th>\n",
       "      <th>bigram_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;p&gt;</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>det</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>&lt;p&gt; The</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>det</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;p&gt;</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Santa</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>compound</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;p&gt; Santa</td>\n",
       "      <td>The Santa</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Santa</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>compound</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Ana</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>compound</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>The Ana</td>\n",
       "      <td>Santa Ana</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ana</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>compound</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Santa</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>River</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Santa River</td>\n",
       "      <td>Ana River</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>River</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Ana</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>be</td>\n",
       "      <td>VERB</td>\n",
       "      <td>ccomp</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Ana is</td>\n",
       "      <td>River is</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>be</td>\n",
       "      <td>VERB</td>\n",
       "      <td>ccomp</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>River</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>DET</td>\n",
       "      <td>det</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>River a</td>\n",
       "      <td>is a</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>a</td>\n",
       "      <td>DET</td>\n",
       "      <td>det</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>be</td>\n",
       "      <td>VERB</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>major</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>amod</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>is major</td>\n",
       "      <td>a major</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>major</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>amod</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>DET</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>river</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>attr</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a river</td>\n",
       "      <td>major river</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>river</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>attr</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>major</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "      <td>prep</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>major in</td>\n",
       "      <td>river in</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "      <td>prep</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>river</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>southern</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>amod</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>river southern</td>\n",
       "      <td>in southern</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lemma    pos       dep  capital  paragraph_boundary  number  dot  is_punct  \\\n",
       "0    <p>  PUNCT      ROOT        0                   1       0    0         0   \n",
       "1    the    DET       det        1                   0       0    0         0   \n",
       "2  Santa  PROPN  compound        1                   0       0    0         0   \n",
       "3    Ana  PROPN  compound        1                   0       0    0         0   \n",
       "4  River  PROPN     nsubj        1                   0       0    0         0   \n",
       "5     be   VERB     ccomp        0                   0       0    0         0   \n",
       "6      a    DET       det        0                   0       0    0         0   \n",
       "7  major    ADJ      amod        0                   0       0    0         0   \n",
       "8  river   NOUN      attr        0                   0       0    0         0   \n",
       "9     in    ADP      prep        0                   0       0    0         0   \n",
       "\n",
       "  prev_lemma prev_token_pos     ...      prev_paragraph_boundary  \\\n",
       "0                               ...                            0   \n",
       "1        <p>          PUNCT     ...                            1   \n",
       "2        the            DET     ...                            0   \n",
       "3      Santa          PROPN     ...                            0   \n",
       "4        Ana          PROPN     ...                            0   \n",
       "5      River          PROPN     ...                            0   \n",
       "6         be           VERB     ...                            0   \n",
       "7          a            DET     ...                            0   \n",
       "8      major            ADJ     ...                            0   \n",
       "9      river           NOUN     ...                            0   \n",
       "\n",
       "   prev_capital  next_lemma next_token_pos  next_dep next_paragraph_boundary  \\\n",
       "0             0         the            DET       det                       0   \n",
       "1             0       Santa          PROPN  compound                       0   \n",
       "2             1         Ana          PROPN  compound                       0   \n",
       "3             1       River          PROPN     nsubj                       0   \n",
       "4             1          be           VERB     ccomp                       0   \n",
       "5             1           a            DET       det                       0   \n",
       "6             0       major            ADJ      amod                       0   \n",
       "7             0       river           NOUN      attr                       0   \n",
       "8             0          in            ADP      prep                       0   \n",
       "9             0    southern            ADJ      amod                       0   \n",
       "\n",
       "   next_capital       neighbors       bigram bigram_group  \n",
       "0             1                      <p> The               \n",
       "1             1       <p> Santa    The Santa               \n",
       "2             1         The Ana    Santa Ana               \n",
       "3             1     Santa River    Ana River               \n",
       "4             0          Ana is     River is               \n",
       "5             0         River a         is a               \n",
       "6             0        is major      a major               \n",
       "7             0         a river  major river               \n",
       "8             0        major in     river in               \n",
       "9             0  river southern  in southern               \n",
       "\n",
       "[10 rows x 21 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def processed(docs):\n",
    "    \n",
    "    tokens = []\n",
    "    for doc in docs:\n",
    "        doc_tokens = [(token.text, token.lemma_, token.pos_, token.dep_) for token in nlp(\" \".join(doc))]\n",
    "        tokens.extend(doc_tokens)\n",
    "    \n",
    "    df = pd.DataFrame(tokens, columns=['token', 'lemma', 'pos', 'dep'])\n",
    "    df['capital'] = df['token'].map(lambda t: 1 if t.istitle() else 0)\n",
    "    df['paragraph_boundary'] = df['token'].map(lambda t: 1 if t in ('<p>', '</p>') else 0)\n",
    "    df['number'] = df['token'].map(lambda t: 1 if t.isnumeric() else 0)\n",
    "    df['dot'] = df['token'].map(lambda t: 1 if t == '.' else 0)\n",
    "    df['is_punct'] = df['token'].map(lambda t: 1 if str(t) in string.punctuation else 0)\n",
    "    \n",
    "    df['prev_lemma'] = df['lemma'].shift(1)\n",
    "    df['prev_token_pos'] = df['pos'].shift(1)\n",
    "    df['prev_dep'] = df['dep'].shift(1)\n",
    "    df['prev_paragraph_boundary'] = df['token'].shift(1).map(lambda t: 1 if t in ('<p>', '</p>') else 0)\n",
    "    df['prev_capital'] = df['token'].shift(1).map(lambda t: 1 if str(t).istitle() else 0)\n",
    "    \n",
    "    df['next_lemma'] = df['lemma'].shift(-1)\n",
    "    df['next_token_pos'] = df['pos'].shift(-1)\n",
    "    df['next_dep'] = df['dep'].shift(-1)\n",
    "    df['next_paragraph_boundary'] = df['token'].shift(-1).map(lambda t: 1 if t in ('<p>', '</p>') else 0)\n",
    "    df['next_capital'] = df['token'].shift(-1).map(lambda t: 1 if str(t).istitle() else 0)\n",
    "    \n",
    "    df['neighbors'] = df['token'].shift(1) + ' ' + df['token'].shift(-1)\n",
    "    \n",
    "    # Feature get frequent trigams list\n",
    "    df['bigram'] = df['token'] + ' ' + df['token'].shift(-1)\n",
    "    df['bigram_group'] = df['bigram'].map(lambda x: transform_num(trigrams_dict.get(x)))\n",
    "    \n",
    "    return df.fillna('').drop('token', axis=1)\n",
    "\n",
    "processed(X_train_).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = DictVectorizer()\n",
    "\n",
    "X = processed(X_train_)\n",
    "X_train = vectorizer.fit_transform(X.to_dict('records'))\n",
    "\n",
    "X = processed(X_test_)\n",
    "X_test = vectorizer.transform(X.to_dict('records'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.981     0.995     0.988    345759\n",
      "           1      0.880     0.666     0.758     19470\n",
      "\n",
      "   micro avg      0.977     0.977     0.977    365229\n",
      "   macro avg      0.931     0.831     0.873    365229\n",
      "weighted avg      0.976     0.977     0.976    365229\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lrc = LogisticRegression(random_state=42, solver=\"lbfgs\", max_iter=1000)\n",
    "\n",
    "lrc.fit(X_train, y_train)\n",
    "y_pred = lrc.predict(X_test)\n",
    "\n",
    "print(classification_report(y_true=y_test, y_pred=y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "add a feature trigrams\n",
    "\n",
    "            precision    recall  f1-score   support\n",
    "\n",
    "           0      0.982     0.994     0.988    345759\n",
    "           1      0.869     0.670     0.756     19470\n",
    "\n",
    "   micro avg      0.977     0.977     0.977    365229\n",
    "   macro avg      0.925     0.832     0.872    365229\n",
    "weighted avg      0.976     0.977     0.976    365229"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False      0.993     0.992     0.992      4942\n",
      "        True      0.744     0.787     0.765       155\n",
      "\n",
      "   micro avg      0.985     0.985     0.985      5097\n",
      "   macro avg      0.869     0.889     0.879      5097\n",
      "weighted avg      0.986     0.985     0.985      5097\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('Data/run-on-test.json') as file:\n",
    "    data = json.load(file)\n",
    "    data = [[['<p>', False]] + i + [['</p>', False]] for i in data]\n",
    "    \n",
    "X_val_, y_val = prepair_X_y(data)\n",
    "y_val =[token for x in y_val for token in x]\n",
    "X = processed(X_val_)\n",
    "X_val = vectorizer.transform(X.to_dict('records'))\n",
    "\n",
    "y_pred = lrc.predict(X_val)\n",
    "\n",
    "print(classification_report(y_true=y_val, y_pred=y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Побудувана модель логістичної регресії з такими кінцевими ознаками:\n",
    "1. lemma\n",
    "2. pos\n",
    "3. dep\n",
    "4. capital\n",
    "5. paragraph_boundary\n",
    "6. number\n",
    "7. dot\n",
    "8. is_punct\n",
    "9. prev_lemma\n",
    "10. prev_token_pos\n",
    "11. prev_dep\n",
    "12. prev_paragraph_boundary\n",
    "13. prev_capital\n",
    "14. next_lemma\n",
    "15. next_token_pos\n",
    "16. next_paragraph_boundary\n",
    "17. next_capital\n",
    "18. neighbors\n",
    "18. bigram\n",
    "19. bigram_group\n",
    "\n",
    "Класифікувати виключно на лемах дало кращу якість. Для класифікації чи після даного слова треба ставити крапку взяла ознаки попереднього, поточного і наступного слів. Перевірила чи покращиться модель, якщо взяти 2ге слово зліва, якість не покращилась. І додатково знайшла датасет частих триграм, з них використала тільки ті триграми, які мають 2им елементом крапку і на цьому базуючись зробила ознаку bigram_group."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Olha Sempi Env",
   "language": "python",
   "name": "olhasempienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
