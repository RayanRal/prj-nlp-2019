{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from keras import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout, LSTM, TimeDistributed, Activation, Bidirectional\n",
    "from keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_file = './data/numberbatch-en-17.06.txt'\n",
    "\n",
    "cn_vectors = KeyedVectors.load_word2vec_format(cn_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"'s\", \" is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[”“’‘'-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "9391948\n",
      "9056654\n"
     ]
    }
   ],
   "source": [
    "### Read/process inputs\n",
    "input_path = './raw_data/'\n",
    "all_data_sents = []\n",
    "\n",
    "files = [join(input_path, f) for f in listdir(input_path) if isfile(join(input_path, f))]\n",
    "for filename in files:\n",
    "    if filename.endswith('.txt'):\n",
    "        with open(filename, encoding='utf-8', errors='ignore') as f:\n",
    "            text = f.read().lower()\n",
    "            text = text.replace('\\n\\n', '\\n')\\\n",
    "                .replace('\"', '')\\\n",
    "                .replace('-', ' - ')\\\n",
    "                .replace(\"\\\"\", '')\\\n",
    "                .replace(\"...\", \"\")\\\n",
    "                .replace(\"…\", \"\")\\\n",
    "                .replace(\"—\", \"\")\n",
    "#                 .replace(\"'\", \"'\")\n",
    "            text = ' '.join(text.split(\"\\n\"))\n",
    "            #.replace('. ', ' . ')\n",
    "#             sentences = re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', text)\n",
    "            all_data_sents.append(text)\n",
    "    \n",
    "print(len(all_data_sents))\n",
    "\n",
    "all_data_string = ' '.join(all_data_sents)\n",
    "print(len(all_data_string))\n",
    "\n",
    "all_data_string = clean_text(all_data_string)\n",
    "print(len(all_data_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters: change to experiment different configurations\n",
    "SEQUENCE_LEN = 20\n",
    "MIN_WORD_FREQUENCY = 2\n",
    "STEP = 1\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length in words: 1763818\n",
      "Unique words before ignoring: 24372\n",
      "Ignoring words with frequency < 2\n",
      "Unique words after ignoring: 17443\n"
     ]
    }
   ],
   "source": [
    "text = all_data_string\n",
    "\n",
    "text_in_words = [w for w in text.split(' ') if w.strip() != '' or w == '\\n']\n",
    "print('Corpus length in words:', len(text_in_words))\n",
    "\n",
    "# Calculate word frequency\n",
    "word_freq = {}\n",
    "for word in text_in_words:\n",
    "    word_freq[word] = word_freq.get(word, 0) + 1\n",
    "\n",
    "ignored_words = set()\n",
    "for k, v in word_freq.items():\n",
    "    if word_freq[k] < MIN_WORD_FREQUENCY:\n",
    "        ignored_words.add(k)\n",
    "        \n",
    "words = set(text_in_words)\n",
    "print('Unique words before ignoring:', len(words))\n",
    "print('Ignoring words with frequency <', MIN_WORD_FREQUENCY)\n",
    "words = sorted(set(words) - ignored_words)\n",
    "print('Unique words after ignoring:', len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(words) + 1\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def vocab_creater(text):\n",
    "\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts([text])\n",
    "    dictionary = tokenizer.word_index\n",
    "  \n",
    "    word2idx = {}\n",
    "    idx2word = {}\n",
    "    for k, v in dictionary.items():\n",
    "        if v < VOCAB_SIZE:\n",
    "            word2idx[k] = v\n",
    "            idx2word[v] = k\n",
    "        if v >= VOCAB_SIZE:\n",
    "            continue\n",
    "          \n",
    "    return word2idx, idx2word, tokenizer\n",
    "\n",
    "word2idx, idx2word, tokenizer = vocab_creater(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_matrix_creater(word2idx):\n",
    "    embedding_matrix = np.zeros((len(word2idx) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word2idx.items():\n",
    "        try:\n",
    "            embedding_vector = cn_vectors.get_vector(word)\n",
    "        except:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_and_split_training_set(sentences_original, next_original, percentage_test=2):\n",
    "    print('Shuffling sentences')\n",
    "\n",
    "    tmp_sentences = []\n",
    "    tmp_next_word = []\n",
    "    for i in np.random.permutation(len(sentences_original)):\n",
    "        tmp_sentences.append(sentences_original[i])\n",
    "        tmp_next_word.append(next_original[i])\n",
    "\n",
    "    cut_index = int(len(sentences_original) * (1.-(percentage_test/100.)))\n",
    "    x_train, x_test = tmp_sentences[:cut_index], tmp_sentences[cut_index:]\n",
    "    y_train, y_test = tmp_next_word[:cut_index], tmp_next_word[cut_index:]\n",
    "\n",
    "    print(\"Size of training set = %d\" % len(x_train))\n",
    "    print(\"Size of test set = %d\" % len(y_test))\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_layer_creater(VOCAB_SIZE, EMBEDDING_DIM, MAX_LEN, embedding_matrix):\n",
    "  \n",
    "    embedding_layer = Embedding(input_dim = VOCAB_SIZE, \n",
    "                                output_dim = EMBEDDING_DIM,\n",
    "                                input_length = MAX_LEN,\n",
    "                                weights = [embedding_matrix],\n",
    "                                trainable = False)\n",
    "    return embedding_layer\n",
    "\n",
    "embedding_matrix = embedding_matrix_creater(word2idx)\n",
    "embedding_layer = embedding_layer_creater(VOCAB_SIZE, EMBEDDING_DIM, SEQUENCE_LEN, embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 1\n",
    "def get_model(dropout=0.2):\n",
    "    print('Building model...')\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "#     model.add(Embedding(input_dim=len(words), output_dim=1024))\n",
    "    model.add(Bidirectional(LSTM(256), input_shape=(SEQUENCE_LEN, 300)))\n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(len(words)))\n",
    "    model.add(Activation('softmax'))\n",
    "    print('Model built')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 2\n",
    "# def get_model(dropout=0.2):\n",
    "#     print('Building model...')\n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(input_dim=len(words), output_dim=1024))\n",
    "#     model.add(LSTM(512, return_sequences=True))\n",
    "#     if dropout > 0:\n",
    "#         model.add(Dropout(dropout))\n",
    "#     model.add(LSTM(512, return_sequences=False))\n",
    "#     if dropout > 0:\n",
    "#         model.add(Dropout(dropout))\n",
    "#     model.add(Dense(len(words)))\n",
    "#     model.add(Activation('softmax'))\n",
    "#     print('Model built')\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions from keras-team/keras/blob/master/examples/lstm_text_generation.py\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('./checkpoints/'):\n",
    "    os.makedirs('./checkpoints/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real code starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: '1',\n",
       " 2: '15th',\n",
       " 3: '[on',\n",
       " 4: 'a',\n",
       " 5: 'aa',\n",
       " 6: 'aback',\n",
       " 7: 'abandon',\n",
       " 8: 'abandoned',\n",
       " 9: 'abandoning',\n",
       " 10: 'abashed',\n",
       " 11: 'abbatoir',\n",
       " 12: 'abed',\n",
       " 13: 'abel',\n",
       " 14: 'abels',\n",
       " 15: 'abhor',\n",
       " 16: 'abide',\n",
       " 17: 'abilities',\n",
       " 18: 'ability',\n",
       " 19: 'ablaze',\n",
       " 20: 'able',\n",
       " 21: 'ably',\n",
       " 22: 'aboard',\n",
       " 23: 'abolish',\n",
       " 24: 'abolished',\n",
       " 25: 'abominably',\n",
       " 26: 'abomination',\n",
       " 27: 'abominations',\n",
       " 28: 'about',\n",
       " 29: 'above',\n",
       " 30: 'abovedecks',\n",
       " 31: 'abreast',\n",
       " 32: 'abroad',\n",
       " 33: 'abrupt',\n",
       " 34: 'abruptly',\n",
       " 35: 'absence',\n",
       " 36: 'absent',\n",
       " 37: 'absently',\n",
       " 38: 'absolute',\n",
       " 39: 'absolutely',\n",
       " 40: 'absolution',\n",
       " 41: 'absolve',\n",
       " 42: 'absorbed',\n",
       " 43: 'absurd',\n",
       " 44: 'absurdly',\n",
       " 45: 'abyss',\n",
       " 46: 'accent',\n",
       " 47: 'accented',\n",
       " 48: 'accents',\n",
       " 49: 'accentuated',\n",
       " 50: 'accept',\n",
       " 51: 'acceptance',\n",
       " 52: 'accepted',\n",
       " 53: 'accepting',\n",
       " 54: 'accepts',\n",
       " 55: 'access',\n",
       " 56: 'accident',\n",
       " 57: 'acclaim',\n",
       " 58: 'acclaimed',\n",
       " 59: 'accommodate',\n",
       " 60: 'accommodating',\n",
       " 61: 'accommodations',\n",
       " 62: 'accompanied',\n",
       " 63: 'accompany',\n",
       " 64: 'accompanying',\n",
       " 65: 'accomplish',\n",
       " 66: 'accomplished',\n",
       " 67: 'accomplishment',\n",
       " 68: 'accord',\n",
       " 69: 'accorded',\n",
       " 70: 'according',\n",
       " 71: 'accordingly',\n",
       " 72: 'accost',\n",
       " 73: 'account',\n",
       " 74: 'accounted',\n",
       " 75: 'accounts',\n",
       " 76: 'accumulation',\n",
       " 77: 'accurate',\n",
       " 78: 'accursed',\n",
       " 79: 'accusation',\n",
       " 80: 'accusations',\n",
       " 81: 'accuse',\n",
       " 82: 'accused',\n",
       " 83: 'accuser',\n",
       " 84: 'accusers',\n",
       " 85: 'accusing',\n",
       " 86: 'accusingly',\n",
       " 87: 'accustomed',\n",
       " 88: 'ache',\n",
       " 89: 'ached',\n",
       " 90: 'aches',\n",
       " 91: 'achieve',\n",
       " 92: 'achieved',\n",
       " 93: 'aching',\n",
       " 94: 'achy',\n",
       " 95: 'acid',\n",
       " 96: 'acidly',\n",
       " 97: 'acknowledge',\n",
       " 98: 'acknowledged',\n",
       " 99: 'acolyte',\n",
       " 100: 'acolytes',\n",
       " 101: 'acorn',\n",
       " 102: 'acorns',\n",
       " 103: 'acquaint',\n",
       " 104: 'acquaintance',\n",
       " 105: 'acquire',\n",
       " 106: 'acquired',\n",
       " 107: 'acquitted',\n",
       " 108: 'acres',\n",
       " 109: 'acrid',\n",
       " 110: 'across',\n",
       " 111: 'act',\n",
       " 112: 'acted',\n",
       " 113: 'acting',\n",
       " 114: 'action',\n",
       " 115: 'actions',\n",
       " 116: 'activity',\n",
       " 117: 'acts',\n",
       " 118: 'actual',\n",
       " 119: 'actually',\n",
       " 120: 'acutely',\n",
       " 121: 'adamant',\n",
       " 122: 'add',\n",
       " 123: 'addam',\n",
       " 124: 'addams',\n",
       " 125: 'addarn',\n",
       " 126: 'added',\n",
       " 127: 'adding',\n",
       " 128: 'addition',\n",
       " 129: 'address',\n",
       " 130: 'addressed',\n",
       " 131: 'adequate',\n",
       " 132: 'adjoined',\n",
       " 133: 'adjoining',\n",
       " 134: 'adjourn',\n",
       " 135: 'adjust',\n",
       " 136: 'adjusted',\n",
       " 137: 'admirable',\n",
       " 138: 'admirably',\n",
       " 139: 'admiral',\n",
       " 140: 'admiration',\n",
       " 141: 'admire',\n",
       " 142: 'admired',\n",
       " 143: 'admirer',\n",
       " 144: 'admirers',\n",
       " 145: 'admiring',\n",
       " 146: 'admission',\n",
       " 147: 'admit',\n",
       " 148: 'admits',\n",
       " 149: 'admitted',\n",
       " 150: 'admittedly',\n",
       " 151: 'admitting',\n",
       " 152: 'adopted',\n",
       " 153: 'adore',\n",
       " 154: 'adorn',\n",
       " 155: 'adorned',\n",
       " 156: 'adultery',\n",
       " 157: 'adults',\n",
       " 158: 'advance',\n",
       " 159: 'advanced',\n",
       " 160: 'advancement',\n",
       " 161: 'advances',\n",
       " 162: 'advancing',\n",
       " 163: 'advantage',\n",
       " 164: 'advantages',\n",
       " 165: 'adventure',\n",
       " 166: 'adventurer',\n",
       " 167: 'adventurers',\n",
       " 168: 'adventures',\n",
       " 169: 'advice',\n",
       " 170: 'advise',\n",
       " 171: 'advised',\n",
       " 172: 'advisors',\n",
       " 173: 'aegon',\n",
       " 174: 'aegons',\n",
       " 175: 'aegor',\n",
       " 176: 'aemon',\n",
       " 177: 'aemons',\n",
       " 178: 'aenys',\n",
       " 179: 'aerion',\n",
       " 180: 'aerions',\n",
       " 181: 'aeron',\n",
       " 182: 'aerons',\n",
       " 183: 'aerys',\n",
       " 184: 'aeryss',\n",
       " 185: 'aethelmure',\n",
       " 186: 'afar',\n",
       " 187: 'affair',\n",
       " 188: 'affairs',\n",
       " 189: 'affected',\n",
       " 190: 'affection',\n",
       " 191: 'affirmed',\n",
       " 192: 'affixed',\n",
       " 193: 'afflicted',\n",
       " 194: 'affliction',\n",
       " 195: 'afford',\n",
       " 196: 'affront',\n",
       " 197: 'affronted',\n",
       " 198: 'afield',\n",
       " 199: 'afire',\n",
       " 200: 'aflame',\n",
       " 201: 'afloat',\n",
       " 202: 'afoot',\n",
       " 203: 'afore',\n",
       " 204: 'afoul',\n",
       " 205: 'afraid',\n",
       " 206: 'afresh',\n",
       " 207: 'aft',\n",
       " 208: 'after',\n",
       " 209: 'afterdeck',\n",
       " 210: 'afterlife',\n",
       " 211: 'aftermath',\n",
       " 212: 'afternoon',\n",
       " 213: 'afternoons',\n",
       " 214: 'aftertaste',\n",
       " 215: 'afterthought',\n",
       " 216: 'afterward',\n",
       " 217: 'again',\n",
       " 218: 'against',\n",
       " 219: 'age',\n",
       " 220: 'aged',\n",
       " 221: 'agents',\n",
       " 222: 'ages',\n",
       " 223: 'aggar',\n",
       " 224: 'aggo',\n",
       " 225: 'aggos',\n",
       " 226: 'aggrieved',\n",
       " 227: 'aghast',\n",
       " 228: 'agile',\n",
       " 229: 'aging',\n",
       " 230: 'agitated',\n",
       " 231: 'aglitter',\n",
       " 232: 'aglow',\n",
       " 233: 'ago',\n",
       " 234: 'agonizing',\n",
       " 235: 'agony',\n",
       " 236: 'agree',\n",
       " 237: 'agreed',\n",
       " 238: 'agreement',\n",
       " 239: 'agrees',\n",
       " 240: 'aground',\n",
       " 241: 'ah',\n",
       " 242: 'ahai',\n",
       " 243: 'ahead',\n",
       " 244: 'ahhhh',\n",
       " 245: 'ahooo',\n",
       " 246: 'ahooooooooo',\n",
       " 247: 'ahooooooooooooooooooo',\n",
       " 248: 'ahorse',\n",
       " 249: 'ai',\n",
       " 250: 'aid',\n",
       " 251: 'aided',\n",
       " 252: 'aiding',\n",
       " 253: 'aids',\n",
       " 254: 'aieeee',\n",
       " 255: 'ailing',\n",
       " 256: 'aim',\n",
       " 257: 'aimed',\n",
       " 258: 'aiming',\n",
       " 259: 'aimlessly',\n",
       " 260: 'aint',\n",
       " 261: 'air',\n",
       " 262: 'airily',\n",
       " 263: 'airless',\n",
       " 264: 'airy',\n",
       " 265: 'aisle',\n",
       " 266: 'aisles',\n",
       " 267: 'akin',\n",
       " 268: 'alabaster',\n",
       " 269: 'alacrity',\n",
       " 270: 'alan',\n",
       " 271: 'alannys',\n",
       " 272: 'alaquo',\n",
       " 273: 'alaric',\n",
       " 274: 'alarm',\n",
       " 275: 'alarmed',\n",
       " 276: 'alarmingly',\n",
       " 277: 'alas',\n",
       " 278: 'alayaya',\n",
       " 279: 'alayayas',\n",
       " 280: 'alayne',\n",
       " 281: 'alaynes',\n",
       " 282: 'albar',\n",
       " 283: 'albeit',\n",
       " 284: 'albett',\n",
       " 285: 'albino',\n",
       " 286: 'alchemist',\n",
       " 287: 'alchemists',\n",
       " 288: 'alcove',\n",
       " 289: 'alcoves',\n",
       " 290: 'alder',\n",
       " 291: 'ale',\n",
       " 292: 'alebelly',\n",
       " 293: 'alehouse',\n",
       " 294: 'alehouses',\n",
       " 295: 'alerie',\n",
       " 296: 'alert',\n",
       " 297: 'alesander',\n",
       " 298: 'alester',\n",
       " 299: 'alf',\n",
       " 300: 'alfyn',\n",
       " 301: 'alien',\n",
       " 302: 'alight',\n",
       " 303: 'alike',\n",
       " 304: 'alios',\n",
       " 305: 'alive',\n",
       " 306: 'all',\n",
       " 307: 'all-for-joffrey',\n",
       " 308: 'alla',\n",
       " 309: 'allar',\n",
       " 310: 'allard',\n",
       " 311: 'allards',\n",
       " 312: 'allbut',\n",
       " 313: 'allegiance',\n",
       " 314: 'alleras',\n",
       " 315: 'alley',\n",
       " 316: 'alleys',\n",
       " 317: 'alliance',\n",
       " 318: 'alliances',\n",
       " 319: 'allied',\n",
       " 320: 'allies',\n",
       " 321: 'alliser',\n",
       " 322: 'allisers',\n",
       " 323: 'allow',\n",
       " 324: 'allowed',\n",
       " 325: 'allowing',\n",
       " 326: 'allows',\n",
       " 327: 'ally',\n",
       " 328: 'allyrion',\n",
       " 329: 'almond',\n",
       " 330: 'almond-shaped',\n",
       " 331: 'almonds',\n",
       " 332: 'almost',\n",
       " 333: 'alms',\n",
       " 334: 'alone',\n",
       " 335: 'along',\n",
       " 336: 'alongside',\n",
       " 337: 'aloof',\n",
       " 338: 'aloud',\n",
       " 339: 'already',\n",
       " 340: 'also',\n",
       " 341: 'altar',\n",
       " 342: 'altars',\n",
       " 343: 'alternated',\n",
       " 344: 'although',\n",
       " 345: 'altogether',\n",
       " 346: 'alvyn',\n",
       " 347: 'always',\n",
       " 348: 'aly',\n",
       " 349: 'alyce',\n",
       " 350: 'alyn',\n",
       " 351: 'alys',\n",
       " 352: 'alysane',\n",
       " 353: 'alysanne',\n",
       " 354: 'alyss',\n",
       " 355: 'alyssa',\n",
       " 356: 'alyssas',\n",
       " 357: 'alyx',\n",
       " 358: 'am',\n",
       " 359: 'amabel',\n",
       " 360: 'amabels',\n",
       " 361: 'amber',\n",
       " 362: 'ambition',\n",
       " 363: 'ambitions',\n",
       " 364: 'ambitious',\n",
       " 365: 'ambrose',\n",
       " 366: 'ambush',\n",
       " 367: 'amend',\n",
       " 368: 'amends',\n",
       " 369: 'amerei',\n",
       " 370: 'amereis',\n",
       " 371: 'amethyst',\n",
       " 372: 'amethysts',\n",
       " 373: 'ami',\n",
       " 374: 'amiable',\n",
       " 375: 'amiably',\n",
       " 376: 'amid',\n",
       " 377: 'amidships',\n",
       " 378: 'amidst',\n",
       " 379: 'amiss',\n",
       " 380: 'among',\n",
       " 381: 'amongst',\n",
       " 382: 'amory',\n",
       " 383: 'amorys',\n",
       " 384: 'amount',\n",
       " 385: 'amounts',\n",
       " 386: 'ample',\n",
       " 387: 'amply',\n",
       " 388: 'amuse',\n",
       " 389: 'amused',\n",
       " 390: 'amusement',\n",
       " 391: 'amusements',\n",
       " 392: 'amusing',\n",
       " 393: 'amuthing',\n",
       " 394: 'an',\n",
       " 395: 'anathema',\n",
       " 396: 'ancestors',\n",
       " 397: 'ancestral',\n",
       " 398: 'anchor',\n",
       " 399: 'anchorage',\n",
       " 400: 'anchored',\n",
       " 401: 'ancient',\n",
       " 402: 'and',\n",
       " 403: 'andahar',\n",
       " 404: 'andal',\n",
       " 405: 'andalos',\n",
       " 406: 'andals',\n",
       " 407: 'andar',\n",
       " 408: 'anders',\n",
       " 409: 'andrew',\n",
       " 410: 'andrey',\n",
       " 411: 'andrik',\n",
       " 412: 'anew',\n",
       " 413: 'angel',\n",
       " 414: 'angels',\n",
       " 415: 'anger',\n",
       " 416: 'angered',\n",
       " 417: 'angers',\n",
       " 418: 'angle',\n",
       " 419: 'angled',\n",
       " 420: 'angling',\n",
       " 421: 'angrier',\n",
       " 422: 'angrily',\n",
       " 423: 'angry',\n",
       " 424: 'anguish',\n",
       " 425: 'anguished',\n",
       " 426: 'anguy',\n",
       " 427: 'animal',\n",
       " 428: 'animals',\n",
       " 429: 'anise',\n",
       " 430: 'ankle',\n",
       " 431: 'ankle-deep',\n",
       " 432: 'ankles',\n",
       " 433: 'annals',\n",
       " 434: 'announce',\n",
       " 435: 'announced',\n",
       " 436: 'announcement',\n",
       " 437: 'annoy',\n",
       " 438: 'annoyance',\n",
       " 439: 'annoyed',\n",
       " 440: 'annoying',\n",
       " 441: 'anointed',\n",
       " 442: 'anointing',\n",
       " 443: 'anon',\n",
       " 444: 'another',\n",
       " 445: 'anothers',\n",
       " 446: 'answer',\n",
       " 447: 'answered',\n",
       " 448: 'answering',\n",
       " 449: 'answers',\n",
       " 450: 'anthill',\n",
       " 451: 'anticipate',\n",
       " 452: 'anticipated',\n",
       " 453: 'anticipation',\n",
       " 454: 'antics',\n",
       " 455: 'antler',\n",
       " 456: 'antlered',\n",
       " 457: 'antlers',\n",
       " 458: 'ants',\n",
       " 459: 'anvil',\n",
       " 460: 'anvil-breaker',\n",
       " 461: 'anvils',\n",
       " 462: 'anxiety',\n",
       " 463: 'anxious',\n",
       " 464: 'anxiously',\n",
       " 465: 'any',\n",
       " 466: 'anya',\n",
       " 467: 'anybody',\n",
       " 468: 'anyhow',\n",
       " 469: 'anymore',\n",
       " 470: 'anyone',\n",
       " 471: 'anyonebut',\n",
       " 472: 'anyones',\n",
       " 473: 'anything',\n",
       " 474: 'anytime',\n",
       " 475: 'anyway',\n",
       " 476: 'anyways',\n",
       " 477: 'anywhere',\n",
       " 478: 'apart',\n",
       " 479: 'apartments',\n",
       " 480: 'ape',\n",
       " 481: 'apex',\n",
       " 482: 'apiece',\n",
       " 483: 'aplenty',\n",
       " 484: 'apologetic',\n",
       " 485: 'apologetically',\n",
       " 486: 'apologies',\n",
       " 487: 'apologize',\n",
       " 488: 'apologized',\n",
       " 489: 'apologizing',\n",
       " 490: 'apology',\n",
       " 491: 'appalled',\n",
       " 492: 'appalling',\n",
       " 493: 'apparent',\n",
       " 494: 'appeal',\n",
       " 495: 'appealed',\n",
       " 496: 'appealing',\n",
       " 497: 'appeals',\n",
       " 498: 'appear',\n",
       " 499: 'appearance',\n",
       " 500: 'appeared',\n",
       " 501: 'appearing',\n",
       " 502: 'appears',\n",
       " 503: 'appease',\n",
       " 504: 'appeased',\n",
       " 505: 'appended',\n",
       " 506: 'appetite',\n",
       " 507: 'appetites',\n",
       " 508: 'appetizing',\n",
       " 509: 'applauding',\n",
       " 510: 'applause',\n",
       " 511: 'apple',\n",
       " 512: 'apple-eater',\n",
       " 513: 'applecakes',\n",
       " 514: 'apples',\n",
       " 515: 'applesauce',\n",
       " 516: 'applied',\n",
       " 517: 'apply',\n",
       " 518: 'appointed',\n",
       " 519: 'appointment',\n",
       " 520: 'appointments',\n",
       " 521: 'appraising',\n",
       " 522: 'appreciate',\n",
       " 523: 'apprehension',\n",
       " 524: 'apprehensive',\n",
       " 525: 'apprentice',\n",
       " 526: 'apprenticed',\n",
       " 527: 'approach',\n",
       " 528: 'approached',\n",
       " 529: 'approaches',\n",
       " 530: 'approaching',\n",
       " 531: 'appropriate',\n",
       " 532: 'approval',\n",
       " 533: 'approve',\n",
       " 534: 'approved',\n",
       " 535: 'approving',\n",
       " 536: 'apricot',\n",
       " 537: 'apron',\n",
       " 538: 'apt',\n",
       " 539: 'ar',\n",
       " 540: 'arakh',\n",
       " 541: 'arakhs',\n",
       " 542: 'arbor',\n",
       " 543: 'arbors',\n",
       " 544: 'arc',\n",
       " 545: 'arcade',\n",
       " 546: 'arced',\n",
       " 547: 'arch',\n",
       " 548: 'arched',\n",
       " 549: 'archer',\n",
       " 550: 'archers',\n",
       " 551: 'archery',\n",
       " 552: 'arches',\n",
       " 553: 'archibald',\n",
       " 554: 'archmaester',\n",
       " 555: 'archmaesters',\n",
       " 556: 'archon',\n",
       " 557: 'archons',\n",
       " 558: 'archway',\n",
       " 559: 'archways',\n",
       " 560: 'arcing',\n",
       " 561: 'arcs',\n",
       " 562: 'ardent',\n",
       " 563: 'ardor',\n",
       " 564: 'are',\n",
       " 565: 'area',\n",
       " 566: 'areas',\n",
       " 567: 'arent',\n",
       " 568: 'areo',\n",
       " 569: 'argue',\n",
       " 570: 'argued',\n",
       " 571: 'arguing',\n",
       " 572: 'argument',\n",
       " 573: 'arguments',\n",
       " 574: 'arianne',\n",
       " 575: 'ariannes',\n",
       " 576: 'aright',\n",
       " 577: 'arise',\n",
       " 578: 'arisen',\n",
       " 579: 'arm',\n",
       " 580: 'armbands',\n",
       " 581: 'armed',\n",
       " 582: 'armen',\n",
       " 583: 'armful',\n",
       " 584: 'armies',\n",
       " 585: 'arming',\n",
       " 586: 'armor',\n",
       " 587: 'armored',\n",
       " 588: 'armorer',\n",
       " 589: 'armorers',\n",
       " 590: 'armory',\n",
       " 591: 'armpit',\n",
       " 592: 'armpits',\n",
       " 593: 'arms',\n",
       " 594: 'army',\n",
       " 595: 'arnolf',\n",
       " 596: 'arnolfs',\n",
       " 597: 'aroma',\n",
       " 598: 'aron',\n",
       " 599: 'arons',\n",
       " 600: 'arose',\n",
       " 601: 'around',\n",
       " 602: 'arousal',\n",
       " 603: 'arouse',\n",
       " 604: 'aroused',\n",
       " 605: 'arousing',\n",
       " 606: 'arrange',\n",
       " 607: 'arranged',\n",
       " 608: 'arrangement',\n",
       " 609: 'arrangements',\n",
       " 610: 'arranging',\n",
       " 611: 'array',\n",
       " 612: 'arrayed',\n",
       " 613: 'arrest',\n",
       " 614: 'arrested',\n",
       " 615: 'arrival',\n",
       " 616: 'arrive',\n",
       " 617: 'arrived',\n",
       " 618: 'arrives',\n",
       " 619: 'arriving',\n",
       " 620: 'arrogance',\n",
       " 621: 'arrogant',\n",
       " 622: 'arron',\n",
       " 623: 'arrons',\n",
       " 624: 'arrow',\n",
       " 625: 'arrowhead',\n",
       " 626: 'arrowheads',\n",
       " 627: 'arrows',\n",
       " 628: 'arrowslet',\n",
       " 629: 'arry',\n",
       " 630: 'arryk',\n",
       " 631: 'arryn',\n",
       " 632: 'arryns',\n",
       " 633: 'arse',\n",
       " 634: 'arsenal',\n",
       " 635: 'arses',\n",
       " 636: 'arson',\n",
       " 637: 'arstan',\n",
       " 638: 'art',\n",
       " 639: 'artfully',\n",
       " 640: 'arthor',\n",
       " 641: 'arthur',\n",
       " 642: 'artifice',\n",
       " 643: 'artos',\n",
       " 644: 'arts',\n",
       " 645: 'arwood',\n",
       " 646: 'arya',\n",
       " 647: 'aryas',\n",
       " 648: 'arys',\n",
       " 649: 'aryss',\n",
       " 650: 'as',\n",
       " 651: 'ascend',\n",
       " 652: 'ascended',\n",
       " 653: 'ascending',\n",
       " 654: 'ascent',\n",
       " 655: 'ascertain',\n",
       " 656: 'ash',\n",
       " 657: 'ash-and-iron',\n",
       " 658: 'asha',\n",
       " 659: 'ashamed',\n",
       " 660: 'ashara',\n",
       " 661: 'asharas',\n",
       " 662: 'ashas',\n",
       " 663: 'ashemark',\n",
       " 664: 'ashen',\n",
       " 665: 'ashes',\n",
       " 666: 'ashford',\n",
       " 667: 'ashore',\n",
       " 668: 'aside',\n",
       " 669: 'ask',\n",
       " 670: 'askance',\n",
       " 671: 'asked',\n",
       " 672: 'askew',\n",
       " 673: 'asking',\n",
       " 674: 'asks',\n",
       " 675: 'asleep',\n",
       " 676: 'aspect',\n",
       " 677: 'aspects',\n",
       " 678: 'aspire',\n",
       " 679: 'aspired',\n",
       " 680: 'aspires',\n",
       " 681: 'ass',\n",
       " 682: 'assail',\n",
       " 683: 'assailed',\n",
       " 684: 'assassin',\n",
       " 685: 'assassins',\n",
       " 686: 'assault',\n",
       " 687: 'assaulted',\n",
       " 688: 'assemble',\n",
       " 689: 'assembled',\n",
       " 690: 'assembling',\n",
       " 691: 'assembly',\n",
       " 692: 'assent',\n",
       " 693: 'asshai',\n",
       " 694: 'asshaii',\n",
       " 695: 'assigned',\n",
       " 696: 'assist',\n",
       " 697: 'assistance',\n",
       " 698: 'assume',\n",
       " 699: 'assumed',\n",
       " 700: 'assuming',\n",
       " 701: 'assurances',\n",
       " 702: 'assure',\n",
       " 703: 'assured',\n",
       " 704: 'assures',\n",
       " 705: 'ast',\n",
       " 706: 'astapor',\n",
       " 707: 'astapori',\n",
       " 708: 'astapors',\n",
       " 709: 'astern',\n",
       " 710: 'astonished',\n",
       " 711: 'astonishing',\n",
       " 712: 'astonishment',\n",
       " 713: 'astray',\n",
       " 714: 'astride',\n",
       " 715: 'asunder',\n",
       " 716: 'aswarm',\n",
       " 717: 'aswirl',\n",
       " 718: 'at',\n",
       " 719: 'atangle',\n",
       " 720: 'ate',\n",
       " 721: 'athwart',\n",
       " 722: 'atone',\n",
       " 723: 'atoned',\n",
       " 724: 'atonement',\n",
       " 725: 'atop',\n",
       " 726: 'atrocities',\n",
       " 727: 'attach',\n",
       " 728: 'attached',\n",
       " 729: 'attaching',\n",
       " 730: 'attack',\n",
       " 731: 'attacked',\n",
       " 732: 'attacker',\n",
       " 733: 'attackers',\n",
       " 734: 'attacking',\n",
       " 735: 'attacks',\n",
       " 736: 'attainder',\n",
       " 737: 'attainted',\n",
       " 738: 'attempt',\n",
       " 739: 'attempted',\n",
       " 740: 'attempts',\n",
       " 741: 'attend',\n",
       " 742: 'attendance',\n",
       " 743: 'attendant',\n",
       " 744: 'attendants',\n",
       " 745: 'attended',\n",
       " 746: 'attending',\n",
       " 747: 'attends',\n",
       " 748: 'attention',\n",
       " 749: 'attentions',\n",
       " 750: 'attentively',\n",
       " 751: 'attest',\n",
       " 752: 'attract',\n",
       " 753: 'attractive',\n",
       " 754: 'auburn',\n",
       " 755: 'auction',\n",
       " 756: 'auctioneer',\n",
       " 757: 'audible',\n",
       " 758: 'audience',\n",
       " 759: 'audiences',\n",
       " 760: 'aught',\n",
       " 761: 'augment',\n",
       " 762: 'aunt',\n",
       " 763: 'aunts',\n",
       " 764: 'aurane',\n",
       " 765: 'aurochs',\n",
       " 766: 'auspicious',\n",
       " 767: 'austere',\n",
       " 768: 'authority',\n",
       " 769: 'autumn',\n",
       " 770: 'autumnal',\n",
       " 771: 'autumns',\n",
       " 772: 'avail',\n",
       " 773: 'available',\n",
       " 774: 'avalanche',\n",
       " 775: 'avarice',\n",
       " 776: 'avenge',\n",
       " 777: 'avenged',\n",
       " 778: 'avenging',\n",
       " 779: 'avenues',\n",
       " 780: 'avert',\n",
       " 781: 'averted',\n",
       " 782: 'avid',\n",
       " 783: 'avoid',\n",
       " 784: 'avoided',\n",
       " 785: 'avoiding',\n",
       " 786: 'await',\n",
       " 787: 'awaited',\n",
       " 788: 'awaiting',\n",
       " 789: 'awaits',\n",
       " 790: 'awake',\n",
       " 791: 'awaken',\n",
       " 792: 'awakened',\n",
       " 793: 'aware',\n",
       " 794: 'awash',\n",
       " 795: 'away',\n",
       " 796: 'awe',\n",
       " 797: 'awed',\n",
       " 798: 'awful',\n",
       " 799: 'awfully',\n",
       " 800: 'awhile',\n",
       " 801: 'awkward',\n",
       " 802: 'awkwardly',\n",
       " 803: 'awning',\n",
       " 804: 'awnings',\n",
       " 805: 'awoke',\n",
       " 806: 'awry',\n",
       " 807: 'axe',\n",
       " 808: 'axehead',\n",
       " 809: 'axell',\n",
       " 810: 'axeman',\n",
       " 811: 'axemen',\n",
       " 812: 'axes',\n",
       " 813: 'aye',\n",
       " 814: 'azor',\n",
       " 815: 'azure',\n",
       " 816: 'azzak',\n",
       " 817: 'b-b-bastard',\n",
       " 818: 'b-b-blood',\n",
       " 819: 'b-bronn',\n",
       " 820: 'b-but',\n",
       " 821: 'babble',\n",
       " 822: 'babbling',\n",
       " 823: 'babe',\n",
       " 824: 'babes',\n",
       " 825: 'babies',\n",
       " 826: 'baby',\n",
       " 827: 'babys',\n",
       " 828: 'back',\n",
       " 829: 'back-to-back',\n",
       " 830: 'backed',\n",
       " 831: 'backhand',\n",
       " 832: 'backing',\n",
       " 833: 'backplate',\n",
       " 834: 'backs',\n",
       " 835: 'backslash',\n",
       " 836: 'backswing',\n",
       " 837: 'backward',\n",
       " 838: 'backwards',\n",
       " 839: 'bacon',\n",
       " 840: 'bad',\n",
       " 841: 'bad-tempered',\n",
       " 842: 'badbrother',\n",
       " 843: 'bade',\n",
       " 844: 'badge',\n",
       " 845: 'badger',\n",
       " 846: 'badgers',\n",
       " 847: 'badges',\n",
       " 848: 'badly',\n",
       " 849: 'bael',\n",
       " 850: 'baelish',\n",
       " 851: 'baelor',\n",
       " 852: 'baelors',\n",
       " 853: 'baels',\n",
       " 854: 'baffled',\n",
       " 855: 'bag',\n",
       " 856: 'baggage',\n",
       " 857: 'baggy',\n",
       " 858: 'bags',\n",
       " 859: 'bah',\n",
       " 860: 'bailey',\n",
       " 861: 'bailiffs',\n",
       " 862: 'bait',\n",
       " 863: 'baiting',\n",
       " 864: 'bake',\n",
       " 865: 'baked',\n",
       " 866: 'baker',\n",
       " 867: 'bakers',\n",
       " 868: 'baking',\n",
       " 869: 'balance',\n",
       " 870: 'balanced',\n",
       " 871: 'balancing',\n",
       " 872: 'balaq',\n",
       " 873: 'balaqs',\n",
       " 874: 'balconies',\n",
       " 875: 'balcony',\n",
       " 876: 'bald',\n",
       " 877: 'balding',\n",
       " 878: 'baldness',\n",
       " 879: 'bale',\n",
       " 880: 'baleful',\n",
       " 881: 'balerion',\n",
       " 882: 'bales',\n",
       " 883: 'balk',\n",
       " 884: 'balked',\n",
       " 885: 'balking',\n",
       " 886: 'ball',\n",
       " 887: 'ballabar',\n",
       " 888: 'ballad',\n",
       " 889: 'ballads',\n",
       " 890: 'ballast',\n",
       " 891: 'balled',\n",
       " 892: 'ballroom',\n",
       " 893: 'balls',\n",
       " 894: 'balm',\n",
       " 895: 'balman',\n",
       " 896: 'balmans',\n",
       " 897: 'balon',\n",
       " 898: 'balons',\n",
       " 899: 'balustrade',\n",
       " 900: 'ban',\n",
       " 901: 'band',\n",
       " 902: 'bandage',\n",
       " 903: 'bandaged',\n",
       " 904: 'bandages',\n",
       " 905: 'banded',\n",
       " 906: 'bands',\n",
       " 907: 'bandy',\n",
       " 908: 'bandy-legged',\n",
       " 909: 'bane',\n",
       " 910: 'banefort',\n",
       " 911: 'bang',\n",
       " 912: 'banged',\n",
       " 913: 'banging',\n",
       " 914: 'banish',\n",
       " 915: 'banished',\n",
       " 916: 'bank',\n",
       " 917: 'banker',\n",
       " 918: 'bankers',\n",
       " 919: 'banks',\n",
       " 920: 'bannen',\n",
       " 921: 'banner',\n",
       " 922: 'banner-bearers',\n",
       " 923: 'bannerman',\n",
       " 924: 'bannermen',\n",
       " 925: 'banners',\n",
       " 926: 'banquet',\n",
       " 927: 'bantam',\n",
       " 928: 'banter',\n",
       " 929: 'bar',\n",
       " 930: 'baratheon',\n",
       " 931: 'baratheons',\n",
       " 932: 'barba',\n",
       " 933: 'barbarian',\n",
       " 934: 'barbarians',\n",
       " 935: 'barbaric',\n",
       " 936: 'barbas',\n",
       " 937: 'barbed',\n",
       " 938: 'barber',\n",
       " 939: 'barber-surgeons',\n",
       " 940: 'barbers',\n",
       " 941: 'barbican',\n",
       " 942: 'barbrey',\n",
       " 943: 'barbs',\n",
       " 944: 'bard',\n",
       " 945: 'barded',\n",
       " 946: 'barding',\n",
       " 947: 'bardings',\n",
       " 948: 'bards',\n",
       " 949: 'bare',\n",
       " 950: 'bare-chested',\n",
       " 951: 'bareback',\n",
       " 952: 'bared',\n",
       " 953: 'barefoot',\n",
       " 954: 'bareheaded',\n",
       " 955: 'barely',\n",
       " 956: 'bargain',\n",
       " 957: 'bargained',\n",
       " 958: 'barge',\n",
       " 959: 'bargeman',\n",
       " 960: 'barges',\n",
       " 961: 'baring',\n",
       " 962: 'bark',\n",
       " 963: 'barked',\n",
       " 964: 'barking',\n",
       " 965: 'barks',\n",
       " 966: 'barley',\n",
       " 967: 'barleycorn',\n",
       " 968: 'barmen',\n",
       " 969: 'barn',\n",
       " 970: 'barnacles',\n",
       " 971: 'barns',\n",
       " 972: 'barra',\n",
       " 973: 'barracks',\n",
       " 974: 'barre',\n",
       " 975: 'barred',\n",
       " 976: 'barrel',\n",
       " 977: 'barrel-chested',\n",
       " 978: 'barrel-vaulted',\n",
       " 979: 'barrels',\n",
       " 980: 'barren',\n",
       " 981: 'barricade',\n",
       " 982: 'barristan',\n",
       " 983: 'barristans',\n",
       " 984: 'barrow',\n",
       " 985: 'barrow-ton',\n",
       " 986: 'barrowlands',\n",
       " 987: 'barrows',\n",
       " 988: 'barrowton',\n",
       " 989: 'bars',\n",
       " 990: 'barsena',\n",
       " 991: 'barsenas',\n",
       " 992: 'barth',\n",
       " 993: 'barths',\n",
       " 994: 'bartimus',\n",
       " 995: 'basalt',\n",
       " 996: 'base',\n",
       " 997: 'basebom',\n",
       " 998: 'baseborn',\n",
       " 999: 'baser',\n",
       " 1000: 'basilisk',\n",
       " ...}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word_indices = dict((c, i) for i, c in enumerate(words))\n",
    "# indices_word = dict((i, c) for i, c in enumerate(words))\n",
    "word2idx\n",
    "idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    examples_file.write('\\n----- Generating text after Epoch: %d\\n' % epoch)\n",
    "\n",
    "    # Randomly pick a seed sequence\n",
    "    seed_index = np.random.randint(len(sentences+sentences_test))\n",
    "    seed = (sentences+sentences_test)[seed_index]\n",
    "\n",
    "    for diversity in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.9, 1.0]:\n",
    "        sentence = seed\n",
    "        examples_file.write('----- Diversity:' + str(diversity) + '\\n')\n",
    "        examples_file.write('----- Generating with seed:\\n\"' + ' '.join(sentence) + '\"\\n')\n",
    "        examples_file.write(' '.join(sentence))\n",
    "\n",
    "        for i in range(50):\n",
    "            x_pred = np.zeros((1, SEQUENCE_LEN))\n",
    "            for t, word in enumerate(sentence):\n",
    "                x_pred[0, t] = word2idx[word]\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_word = idx2word[next_index]\n",
    "\n",
    "            sentence = sentence[1:]\n",
    "            sentence.append(next_word)\n",
    "\n",
    "            examples_file.write(\" \"+next_word)\n",
    "        examples_file.write('\\n')\n",
    "    examples_file.write('='*80 + '\\n')\n",
    "    examples_file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings - Data generator for fit and evaluate\n",
    "def generator(sentence_list, next_word_list, batch_size):\n",
    "    index = 0\n",
    "    while True:\n",
    "        x = np.zeros((batch_size, SEQUENCE_LEN), dtype=np.int32)\n",
    "        y = np.zeros((batch_size), dtype=np.int32)\n",
    "        for i in range(batch_size):\n",
    "            for t, w in enumerate(sentence_list[index % len(sentence_list)]):\n",
    "                x[i, t] = word2idx[w]\n",
    "            y[i] = word2idx[next_word_list[index % len(sentence_list)]]\n",
    "            index = index + 1\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO EMBEDDINGS - Data generator for fit and evaluate\n",
    "def generator(sentence_list, next_word_list, batch_size):\n",
    "    index = 0\n",
    "    while True:\n",
    "        x = np.zeros((batch_size, SEQUENCE_LEN, len(words)), dtype=np.bool)\n",
    "        y = np.zeros((batch_size, len(words)), dtype=np.bool)\n",
    "        for i in range(batch_size):\n",
    "            for t, w in enumerate(sentence_list[index % len(sentence_list)]):\n",
    "                x[i, t, word2idx[w]] = 1\n",
    "            y[i, word2idx[next_word_list[index % len(sentence_list)]]] = 1\n",
    "            index = index + 1\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored sequences: 151057\n",
      "Remaining sequences: 1601086\n"
     ]
    }
   ],
   "source": [
    "# cut the text in semi-redundant sequences of SEQUENCE_LEN words\n",
    "sentences = []\n",
    "next_words = []\n",
    "ignored = 0\n",
    "\n",
    "for i in range(0, len(text_in_words) - SEQUENCE_LEN, STEP):\n",
    "    # Only add the sequences where no word is in ignored_words\n",
    "    if len(set(text_in_words[i: i+SEQUENCE_LEN+1]).intersection(ignored_words)) == 0:\n",
    "        sentences.append(text_in_words[i: i + SEQUENCE_LEN])\n",
    "        next_words.append(text_in_words[i + SEQUENCE_LEN])\n",
    "    else:\n",
    "        ignored = ignored + 1\n",
    "\n",
    "print('Ignored sequences:', ignored)\n",
    "print('Remaining sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling sentences\n",
      "Size of training set = 1569064\n",
      "Size of test set = 32022\n"
     ]
    }
   ],
   "source": [
    "# x, y, x_test, y_test\n",
    "(sentences, next_words), (sentences_test, next_words_test) = shuffle_and_split_training_set(sentences, next_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "WARNING:tensorflow:From /Users/leonid/Projects/prj-nlp-2019/venv/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/leonid/Projects/prj-nlp-2019/venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Model built\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "checkpoint_file_path = \"./checkpoints/LSTM_GRRM-epoch{epoch:03d}-words%d-sequence%d-minfreq%d-\" \\\n",
    "            \"loss{loss:.4f}-acc{acc:.4f}-val_loss{val_loss:.4f}-val_acc{val_acc:.4f}\" % \\\n",
    "            (len(words), SEQUENCE_LEN, MIN_WORD_FREQUENCY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(checkpoint_file_path, monitor='val_acc', save_best_only=False)\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=20)\n",
    "\n",
    "callbacks_list = [checkpoint, print_callback, early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/leonid/Projects/prj-nlp-2019/venv/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/40\n",
      "  243/49034 [..............................] - ETA: 3:19:07 - loss: 7.3692 - acc: 0.0568"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-bc267b8318b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_words_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                     validation_steps=int(len(sentences_test)/BATCH_SIZE) + 1)\n\u001b[0m",
      "\u001b[0;32m~/Projects/prj-nlp-2019/venv/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/prj-nlp-2019/venv/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/prj-nlp-2019/venv/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/prj-nlp-2019/venv/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/prj-nlp-2019/venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/prj-nlp-2019/venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/prj-nlp-2019/venv/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "examples = './examples/examples_1'\n",
    "examples_file = open(examples, \"w\")\n",
    "model.fit_generator(generator(sentences, next_words, BATCH_SIZE),\n",
    "                    steps_per_epoch=int(len(sentences)/BATCH_SIZE) + 1,\n",
    "                    epochs=40,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_data=generator(sentences_test, next_words_test, BATCH_SIZE),\n",
    "                    validation_steps=int(len(sentences_test)/BATCH_SIZE) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"./checkpoints/LSTM_LYRICS-epoch006-words11925-sequence10-minfreq10-loss5.3823-acc0.1647-val_loss5.4348-val_acc0.1620\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
