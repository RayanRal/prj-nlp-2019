{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from keras import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout, LSTM, TimeDistributed, Activation, Bidirectional\n",
    "from keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_file = './data/numberbatch-en-17.06.txt'\n",
    "\n",
    "cn_vectors = KeyedVectors.load_word2vec_format(cn_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"'s\", \" is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[”“’‘'-()\\\"#/@;:<>{}`+=~|!?]\", \"\", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1624570\n",
      "1601176\n"
     ]
    }
   ],
   "source": [
    "### Read/process inputs\n",
    "input_path = './raw_data/'\n",
    "all_data_sents = []\n",
    "\n",
    "files = [join(input_path, f) for f in listdir(input_path) if isfile(join(input_path, f))]\n",
    "for filename in files:\n",
    "    if filename.endswith('.txt'):\n",
    "        with open(filename, encoding='utf-8', errors='ignore') as f:\n",
    "            text = f.read().lower()\n",
    "            text = text.replace('\\n\\n', '\\n')\\\n",
    "                .replace('\"', '')\\\n",
    "                .replace('-', ' - ')\\\n",
    "                .replace(\"\\\"\", '')\\\n",
    "                .replace(\"...\", \"\")\\\n",
    "                .replace(\"…\", \"\")\\\n",
    "                .replace(\"—\", \"\")\\\n",
    "                .replace(\". \", \" . \")\\\n",
    "                .replace(\", \", \" , \")\n",
    "            text = ' '.join(text.split(\"\\n\"))\n",
    "#             sentences = re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', text)\n",
    "            all_data_sents.append(text)\n",
    "    \n",
    "print(len(all_data_sents))\n",
    "\n",
    "all_data_string = ' '.join(all_data_sents)\n",
    "print(len(all_data_string))\n",
    "\n",
    "all_data_string = clean_text(all_data_string)\n",
    "print(len(all_data_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "9609400\n",
      "9534701\n"
     ]
    }
   ],
   "source": [
    "### Read/process inputs\n",
    "input_path_full = './raw_data_input/'\n",
    "all_data_sents_full = []\n",
    "\n",
    "files = [join(input_path_full, f) for f in listdir(input_path_full) if isfile(join(input_path_full, f))]\n",
    "for filename in files:\n",
    "    if filename.endswith('.txt'):\n",
    "        with open(filename, encoding='utf-8', errors='ignore') as f:\n",
    "            text = f.read().lower()\n",
    "            text = text.replace('\\n\\n', '\\n')\\\n",
    "                .replace('\"', '')\\\n",
    "                .replace('-', ' - ')\\\n",
    "                .replace(\"\\\"\", '')\\\n",
    "                .replace(\"...\", \"\")\\\n",
    "                .replace(\"…\", \"\")\\\n",
    "                .replace(\"—\", \"\")\\\n",
    "                .replace('. ', ' . ')\\\n",
    "                .replace(', ', ' , ')\n",
    "            text = ' '.join(text.split(\"\\n\"))\n",
    "#             sentences = re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', text)\n",
    "            all_data_sents_full.append(text)\n",
    "    \n",
    "print(len(all_data_sents_full))\n",
    "\n",
    "all_data_string_full = ' '.join(all_data_sents_full)\n",
    "print(len(all_data_string_full))\n",
    "\n",
    "all_data_string_full = clean_text(all_data_string_full)\n",
    "print(len(all_data_string_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LEN = 25\n",
    "MIN_WORD_FREQUENCY = 5\n",
    "STEP = 1\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length in words: 331517\n",
      "Unique words before ignoring: 14478\n",
      "Ignoring words with frequency < 5\n",
      "Unique words after ignoring: 4546\n"
     ]
    }
   ],
   "source": [
    "text = all_data_string\n",
    "text_full = all_data_string_full\n",
    "\n",
    "text_in_words = [w for w in text.split(' ') if w.strip() != '' or w == '\\n']\n",
    "text_in_words_full = [w for w in text_full.split(' ') if w.strip() != '' or w == '\\n']\n",
    "print('Corpus length in words:', len(text_in_words))\n",
    "\n",
    "# Calculate word frequency\n",
    "word_freq = {}\n",
    "for word in text_in_words:\n",
    "    word_freq[word] = word_freq.get(word, 0) + 1\n",
    "\n",
    "ignored_words = set()\n",
    "for k, v in word_freq.items():\n",
    "    if word_freq[k] < MIN_WORD_FREQUENCY:\n",
    "        ignored_words.add(k)\n",
    "        \n",
    "words = set(text_in_words)\n",
    "print('Unique words before ignoring:', len(words))\n",
    "print('Ignoring words with frequency <', MIN_WORD_FREQUENCY)\n",
    "words = sorted(set(words) - ignored_words)\n",
    "print('Unique words after ignoring:', len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(words) + 1\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# def vocab_creater(text):\n",
    "\n",
    "#     tokenizer = Tokenizer()\n",
    "#     tokenizer.fit_on_texts([text])\n",
    "#     dictionary = tokenizer.word_index\n",
    "  \n",
    "#     word2idx = {}\n",
    "#     idx2word = {}\n",
    "#     for k, v in dictionary.items():\n",
    "#         if v < VOCAB_SIZE:\n",
    "#             word2idx[k] = v\n",
    "#             idx2word[v] = k\n",
    "#         if v >= VOCAB_SIZE:\n",
    "#             continue\n",
    "          \n",
    "#     return word2idx, idx2word, tokenizer\n",
    "\n",
    "# word2idx, idx2word, tokenizer = vocab_creater(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_indices = dict((c, i) for i, c in enumerate(words))\n",
    "indices_word = dict((i, c) for i, c in enumerate(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_matrix_creater(word_indices):\n",
    "    embedding_matrix = np.zeros((len(word_indices) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_indices.items():\n",
    "        try:\n",
    "            embedding_vector = cn_vectors.get_vector(word)\n",
    "        except:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_layer_creater(VOCAB_SIZE, EMBEDDING_DIM, MAX_LEN, embedding_matrix):\n",
    "  \n",
    "    embedding_layer = Embedding(input_dim = VOCAB_SIZE, \n",
    "                                output_dim = EMBEDDING_DIM,\n",
    "                                input_length = MAX_LEN,\n",
    "                                weights = [embedding_matrix],\n",
    "                                trainable = False)\n",
    "    return embedding_layer\n",
    "\n",
    "embedding_matrix = embedding_matrix_creater(word_indices)\n",
    "embedding_layer = embedding_layer_creater(VOCAB_SIZE, EMBEDDING_DIM, SEQUENCE_LEN, embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_and_split_training_set(sentences_original, next_original, percentage_test=20):\n",
    "    print('Shuffling sentences')\n",
    "\n",
    "    tmp_sentences = []\n",
    "    tmp_next_word = []\n",
    "    for i in np.random.permutation(len(sentences_original)):\n",
    "        tmp_sentences.append(sentences_original[i])\n",
    "        tmp_next_word.append(next_original[i])\n",
    "\n",
    "    cut_index = int(len(sentences_original) * (1.-(percentage_test/100.)))\n",
    "    x_train, x_test = tmp_sentences[:cut_index], tmp_sentences[cut_index:]\n",
    "    y_train, y_test = tmp_next_word[:cut_index], tmp_next_word[cut_index:]\n",
    "\n",
    "    print(\"Size of training set = %d\" % len(x_train))\n",
    "    print(\"Size of test set = %d\" % len(y_test))\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 1\n",
    "def get_model(dropout=0.4):\n",
    "    print('Building model...')\n",
    "    model = Sequential()\n",
    "#     model.add(embedding_layer)\n",
    "    model.add(Embedding(input_dim=len(words), output_dim=1024))\n",
    "    model.add(Bidirectional(LSTM(256)))\n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(len(words)))\n",
    "    model.add(Activation('softmax'))\n",
    "    print('Model built')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 2\n",
    "# def get_model(dropout=0.2):\n",
    "#     print('Building model...')\n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(input_dim=len(words), output_dim=1024))\n",
    "#     model.add(LSTM(512, return_sequences=True))\n",
    "#     if dropout > 0:\n",
    "#         model.add(Dropout(dropout))\n",
    "#     model.add(LSTM(512, return_sequences=False))\n",
    "#     if dropout > 0:\n",
    "#         model.add(Dropout(dropout))\n",
    "#     model.add(Dense(len(words)))\n",
    "#     model.add(Activation('softmax'))\n",
    "#     print('Model built')\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions from keras-team/keras/blob/master/examples/lstm_text_generation.py\n",
    "def sample(preds, temperature=None):\n",
    "    if temperature is None:\n",
    "        return np.argmax(preds)\n",
    "    else:\n",
    "        # helper function to sample an index from a probability array\n",
    "        preds = np.asarray(preds).astype('float64')\n",
    "        preds = np.log(preds) / temperature\n",
    "        exp_preds = np.exp(preds)\n",
    "        preds = exp_preds / np.sum(exp_preds)\n",
    "        probas = np.random.multinomial(1, preds, 1)\n",
    "        return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('./checkpoints/'):\n",
    "    os.makedirs('./checkpoints/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real code starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    examples_file.write('\\n----- Generating text after Epoch: %d\\n' % epoch)\n",
    "\n",
    "    # Randomly pick a seed sequence\n",
    "    seed_index = np.random.randint(len(sentences_test))\n",
    "    seed = (sentences_test)[seed_index]\n",
    "\n",
    "    for diversity in [None, 0.3, 0.4, 0.5, 0.6]:\n",
    "        sentence = seed\n",
    "        examples_file.write('----- Diversity:' + str(diversity) + '\\n')\n",
    "        examples_file.write('----- Generating with seed:\\n\"' + ' '.join(sentence) + '\"\\n')\n",
    "        examples_file.write(' '.join(sentence))\n",
    "\n",
    "        for i in range(100):\n",
    "            x_pred = np.zeros((1, SEQUENCE_LEN))\n",
    "            for t, word in enumerate(sentence):\n",
    "                x_pred[0, t] = word_indices[word]\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_word = indices_word[next_index]\n",
    "\n",
    "            sentence = sentence[1:]\n",
    "            sentence.append(next_word)\n",
    "\n",
    "            examples_file.write(\" \"+next_word)\n",
    "        examples_file.write('\\n')\n",
    "    examples_file.write('='*80 + '\\n')\n",
    "    examples_file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings - Data generator for fit and evaluate\n",
    "def generator(sentence_list, next_word_list, batch_size):\n",
    "    index = 0\n",
    "    while True:\n",
    "        x = np.zeros((batch_size, SEQUENCE_LEN), dtype=np.int32)\n",
    "        y = np.zeros((batch_size), dtype=np.int32)\n",
    "        for i in range(batch_size):\n",
    "            for t, w in enumerate(sentence_list[index % len(sentence_list)]):\n",
    "                x[i, t] = word_indices[w]\n",
    "            y[i] = word_indices[next_word_list[index % len(sentence_list)]]\n",
    "            index = index + 1\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings on exit - Data generator for fit and evaluate\n",
    "# def generator(sentence_list, next_word_list, batch_size):\n",
    "#     index = 0\n",
    "#     while True:\n",
    "#         x = np.zeros((batch_size, SEQUENCE_LEN), dtype=np.int32)\n",
    "#         y = np.zeros((batch_size, EMBEDDING_DIM), dtype=np.int32)\n",
    "#         for i in range(batch_size):\n",
    "# #             try:\n",
    "# #                 embedding_vector = cn_vectors.get_vector(word)\n",
    "# #             except:\n",
    "# #                 embedding_vector = None\n",
    "# #             if embedding_vector is not None:\n",
    "# #                 # words not found in embedding index will be all-zeros.\n",
    "# #                 embedding_matrix[i] = embedding_vector\n",
    "#             for t, w in enumerate(sentence_list[index % len(sentence_list)]):\n",
    "#                 x[i, t] = word_indices[w]\n",
    "#             y[i] = np.zeros(EMBEDDING_DIM)\n",
    "#             word = next_word_list[index % len(sentence_list)]\n",
    "#             try:\n",
    "#                 embedding_vector = cn_vectors.get_vector(word)\n",
    "#             except:\n",
    "#                 embedding_vector = None\n",
    "#             if embedding_vector is not None:\n",
    "# #                 # words not found in embedding index will be all-zeros.\n",
    "#                 y[i] = embedding_vector\n",
    "#             index = index + 1\n",
    "#         yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO EMBEDDINGS - Data generator for fit and evaluate\n",
    "# def generator(sentence_list, next_word_list, batch_size):\n",
    "#     index = 0\n",
    "#     while True:\n",
    "#         x = np.zeros((batch_size, SEQUENCE_LEN, len(words)), dtype=np.bool)\n",
    "#         y = np.zeros((batch_size, len(words)), dtype=np.bool)\n",
    "#         for i in range(batch_size):\n",
    "#             for t, w in enumerate(sentence_list[index % len(sentence_list)]):\n",
    "#                 x[i, t, word_indices[w]] = 1\n",
    "#             y[i, word_indices[next_word_list[index % len(sentence_list)]]] = 1\n",
    "#             index = index + 1\n",
    "#         yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_set = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored sequences: 1676344\n",
      "Remaining sequences: 305100\n"
     ]
    }
   ],
   "source": [
    "# cut the text in semi-redundant sequences of SEQUENCE_LEN words\n",
    "sentences = []\n",
    "next_words = []\n",
    "ignored = 0\n",
    "\n",
    "for i in range(0, len(text_in_words_full) - SEQUENCE_LEN, STEP):\n",
    "    # Only add the sequences where no word is in ignored_words\n",
    "    if len(set(text_in_words_full[i: i+SEQUENCE_LEN+1]).intersection(word_set)) == len(set(text_in_words_full[i: i+SEQUENCE_LEN+1])):\n",
    "        sentences.append(text_in_words_full[i: i + SEQUENCE_LEN])\n",
    "        next_words.append(text_in_words_full[i + SEQUENCE_LEN])\n",
    "    else:\n",
    "        ignored = ignored + 1\n",
    "\n",
    "print('Ignored sequences:', ignored)\n",
    "print('Remaining sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling sentences\n",
      "Size of training set = 244080\n",
      "Size of test set = 61020\n"
     ]
    }
   ],
   "source": [
    "# x, y, x_test, y_test\n",
    "(sentences, next_words), (sentences_test, next_words_test) = shuffle_and_split_training_set(sentences, next_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Model built\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "checkpoint_file_path = \"./checkpoints/LSTM_GRRM-epoch{epoch:03d}-words%d-sequence%d-minfreq%d-\" \\\n",
    "            \"loss{loss:.4f}-acc{acc:.4f}-val_loss{val_loss:.4f}-val_acc{val_acc:.4f}\" % \\\n",
    "            (len(words), SEQUENCE_LEN, MIN_WORD_FREQUENCY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(checkpoint_file_path, monitor='val_acc', save_best_only=False)\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "# early_stopping = EarlyStopping(monitor='val_acc', patience=20)\n",
    "\n",
    "callbacks_list = [checkpoint, print_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(\"./checkpoints/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/60\n",
      "954/954 [==============================] - 71s 75ms/step - loss: 5.7680 - acc: 0.0859 - val_loss: 5.3507 - val_acc: 0.1123\n",
      "Epoch 2/60\n",
      "954/954 [==============================] - 69s 72ms/step - loss: 5.2362 - acc: 0.1148 - val_loss: 5.0882 - val_acc: 0.1296\n",
      "Epoch 3/60\n",
      "954/954 [==============================] - 69s 72ms/step - loss: 5.0286 - acc: 0.1261 - val_loss: 4.9522 - val_acc: 0.1394\n",
      "Epoch 4/60\n",
      "954/954 [==============================] - 69s 72ms/step - loss: 4.8826 - acc: 0.1341 - val_loss: 4.8662 - val_acc: 0.1478\n",
      "Epoch 5/60\n",
      "954/954 [==============================] - 69s 73ms/step - loss: 4.7674 - acc: 0.1409 - val_loss: 4.8143 - val_acc: 0.1515\n",
      "Epoch 6/60\n",
      "954/954 [==============================] - 69s 72ms/step - loss: 4.6663 - acc: 0.1467 - val_loss: 4.7822 - val_acc: 0.1545\n",
      "Epoch 7/60\n",
      "954/954 [==============================] - 69s 73ms/step - loss: 4.5792 - acc: 0.1505 - val_loss: 4.7507 - val_acc: 0.1594\n",
      "Epoch 8/60\n",
      "954/954 [==============================] - 69s 72ms/step - loss: 4.5018 - acc: 0.1550 - val_loss: 4.7265 - val_acc: 0.1606\n",
      "Epoch 9/60\n",
      "954/954 [==============================] - 69s 73ms/step - loss: 4.4242 - acc: 0.1590 - val_loss: 4.7186 - val_acc: 0.1625\n",
      "Epoch 10/60\n",
      "954/954 [==============================] - 69s 72ms/step - loss: 4.3467 - acc: 0.1628 - val_loss: 4.7184 - val_acc: 0.1645\n",
      "Epoch 11/60\n",
      "954/954 [==============================] - 69s 72ms/step - loss: 4.2706 - acc: 0.1674 - val_loss: 4.7219 - val_acc: 0.1660\n",
      "Epoch 12/60\n",
      "954/954 [==============================] - 69s 72ms/step - loss: 4.1942 - acc: 0.1714 - val_loss: 4.7370 - val_acc: 0.1658\n",
      "Epoch 13/60\n",
      "954/954 [==============================] - 69s 72ms/step - loss: 4.1218 - acc: 0.1771 - val_loss: 4.7498 - val_acc: 0.1662\n",
      "Epoch 14/60\n",
      "954/954 [==============================] - 69s 72ms/step - loss: 4.0454 - acc: 0.1841 - val_loss: 4.7644 - val_acc: 0.1672\n",
      "Epoch 15/60\n",
      "954/954 [==============================] - 69s 72ms/step - loss: 3.9715 - acc: 0.1915 - val_loss: 4.7807 - val_acc: 0.1659\n",
      "Epoch 16/60\n",
      "954/954 [==============================] - 69s 72ms/step - loss: 3.9024 - acc: 0.1972 - val_loss: 4.7939 - val_acc: 0.1657\n",
      "Epoch 17/60\n",
      "954/954 [==============================] - 69s 72ms/step - loss: 3.8353 - acc: 0.2050 - val_loss: 4.8187 - val_acc: 0.1669\n",
      "Epoch 18/60\n",
      "954/954 [==============================] - 69s 72ms/step - loss: 3.7680 - acc: 0.2123 - val_loss: 4.8395 - val_acc: 0.1669\n",
      "Epoch 19/60\n",
      "954/954 [==============================] - 69s 72ms/step - loss: 3.7126 - acc: 0.2186 - val_loss: 4.8616 - val_acc: 0.1659\n",
      "Epoch 20/60\n",
      "954/954 [==============================] - 69s 72ms/step - loss: 3.6494 - acc: 0.2257 - val_loss: 4.8853 - val_acc: 0.1666\n",
      "Epoch 21/60\n",
      "954/954 [==============================] - 69s 72ms/step - loss: 3.6304 - acc: 0.2277 - val_loss: 4.8903 - val_acc: 0.1656\n",
      "Epoch 22/60\n",
      "954/954 [==============================] - 68s 72ms/step - loss: 3.5993 - acc: 0.2325 - val_loss: 4.9035 - val_acc: 0.1662\n",
      "Epoch 23/60\n",
      "954/954 [==============================] - 69s 72ms/step - loss: 3.5538 - acc: 0.2379 - val_loss: 4.9215 - val_acc: 0.1654\n",
      "Epoch 24/60\n",
      "954/954 [==============================] - 69s 72ms/step - loss: 3.4946 - acc: 0.2449 - val_loss: 4.9474 - val_acc: 0.1650\n",
      "Epoch 25/60\n",
      "954/954 [==============================] - 69s 72ms/step - loss: 3.4435 - acc: 0.2508 - val_loss: 4.9709 - val_acc: 0.1645\n",
      "Epoch 26/60\n",
      "954/954 [==============================] - 69s 72ms/step - loss: 3.3955 - acc: 0.2567 - val_loss: 5.0000 - val_acc: 0.1636\n",
      "Epoch 27/60\n",
      "954/954 [==============================] - 69s 72ms/step - loss: 3.3503 - acc: 0.2632 - val_loss: 5.0162 - val_acc: 0.1637\n",
      "Epoch 28/60\n",
      "954/954 [==============================] - 68s 72ms/step - loss: 3.3012 - acc: 0.2697 - val_loss: 5.0441 - val_acc: 0.1639\n",
      "Epoch 29/60\n",
      "954/954 [==============================] - 69s 72ms/step - loss: 3.2582 - acc: 0.2747 - val_loss: 5.0678 - val_acc: 0.1629\n",
      "Epoch 30/60\n",
      "954/954 [==============================] - 68s 72ms/step - loss: 3.2219 - acc: 0.2800 - val_loss: 5.0933 - val_acc: 0.1618\n",
      "Epoch 31/60\n",
      "954/954 [==============================] - 69s 72ms/step - loss: 3.1855 - acc: 0.2855 - val_loss: 5.1117 - val_acc: 0.1619\n",
      "Epoch 32/60\n",
      "954/954 [==============================] - 68s 72ms/step - loss: 3.1425 - acc: 0.2907 - val_loss: 5.1275 - val_acc: 0.1612\n",
      "Epoch 33/60\n",
      "954/954 [==============================] - 69s 72ms/step - loss: 3.1354 - acc: 0.2902 - val_loss: 5.1354 - val_acc: 0.1608\n",
      "Epoch 34/60\n",
      "954/954 [==============================] - 68s 72ms/step - loss: 3.1161 - acc: 0.2935 - val_loss: 5.1586 - val_acc: 0.1614\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6cb8e955f8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = './examples/examples_1'\n",
    "examples_file = open(examples, \"w\")\n",
    "model.fit_generator(generator(sentences, next_words, BATCH_SIZE),\n",
    "                    steps_per_epoch=int(len(sentences)/BATCH_SIZE) + 1,\n",
    "                    epochs=100,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_data=generator(sentences_test, next_words_test, BATCH_SIZE),\n",
    "                    validation_steps=int(len(sentences_test)/BATCH_SIZE) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(\"./checkpoints/LSTM_LYRICS-epoch006-words11925-sequence10-minfreq10-loss5.3823-acc0.1647-val_loss5.4348-val_acc0.1620\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
