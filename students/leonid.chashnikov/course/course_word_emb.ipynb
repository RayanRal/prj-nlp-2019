{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from keras import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout, LSTM, TimeDistributed, Activation, Bidirectional\n",
    "from keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_file = './data/numberbatch-en-17.06.txt'\n",
    "\n",
    "cn_vectors = KeyedVectors.load_word2vec_format(cn_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = text.replace('\\n\\n', '\\n')\\\n",
    "                .replace('\"', '')\\\n",
    "                .replace('-', ' - ')\\\n",
    "                .replace(\"\\\"\", '')\\\n",
    "                .replace(\"...\", \"\")\\\n",
    "                .replace(\"…\", \"\")\\\n",
    "                .replace(\"—\", \"\")\\\n",
    "                .replace(\". \", \" . \")\\\n",
    "                .replace(\", \", \" , \")\n",
    "    text = ' '.join(text.split(\"\\n\"))\n",
    "    \n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"'s\", \" is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[”“’‘'-()\\\"#/@;:<>{}`+=~|!?]\", \"\", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "9450373\n",
      "9534701\n"
     ]
    }
   ],
   "source": [
    "### Read/process inputs\n",
    "input_path = './raw_data/'\n",
    "all_data_sents = []\n",
    "\n",
    "files = [join(input_path, f) for f in listdir(input_path) if isfile(join(input_path, f))]\n",
    "for filename in files:\n",
    "    if filename.endswith('.txt'):\n",
    "        with open(filename, encoding='utf-8', errors='ignore') as f:\n",
    "            text = f.read().lower()\n",
    "#             text = text.replace('\\n\\n', '\\n')\\\n",
    "#                 .replace('\"', '')\\\n",
    "#                 .replace('-', ' - ')\\\n",
    "#                 .replace(\"\\\"\", '')\\\n",
    "#                 .replace(\"...\", \"\")\\\n",
    "#                 .replace(\"…\", \"\")\\\n",
    "#                 .replace(\"—\", \"\")\\\n",
    "#                 .replace(\". \", \" . \")\\\n",
    "#                 .replace(\", \", \" , \")\n",
    "#             text = ' '.join(text.split(\"\\n\"))\n",
    "#             sentences = re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', text)\n",
    "            all_data_sents.append(text)\n",
    "    \n",
    "print(len(all_data_sents))\n",
    "\n",
    "all_data_string = ' '.join(all_data_sents)\n",
    "print(len(all_data_string))\n",
    "\n",
    "all_data_string = clean_text(all_data_string)\n",
    "print(len(all_data_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LEN = 25\n",
    "MIN_WORD_FREQUENCY = 5\n",
    "STEP = 1\n",
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length in words: 1981469\n",
      "Unique words before ignoring: 32074\n",
      "Ignoring words with frequency < 5\n",
      "Unique words after ignoring: 13166\n"
     ]
    }
   ],
   "source": [
    "text = all_data_string\n",
    "\n",
    "text_in_words = [w for w in text.split(' ') if w.strip() != '' or w == '\\n']\n",
    "print('Corpus length in words:', len(text_in_words))\n",
    "\n",
    "# Calculate word frequency\n",
    "word_freq = {}\n",
    "for word in text_in_words:\n",
    "    word_freq[word] = word_freq.get(word, 0) + 1\n",
    "\n",
    "ignored_words = set()\n",
    "for k, v in word_freq.items():\n",
    "    if word_freq[k] < MIN_WORD_FREQUENCY:\n",
    "        ignored_words.add(k)\n",
    "        \n",
    "words = set(text_in_words)\n",
    "print('Unique words before ignoring:', len(words))\n",
    "print('Ignoring words with frequency <', MIN_WORD_FREQUENCY)\n",
    "words = sorted(set(words) - ignored_words)\n",
    "print('Unique words after ignoring:', len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(words) + 1\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# def vocab_creater(text):\n",
    "\n",
    "#     tokenizer = Tokenizer()\n",
    "#     tokenizer.fit_on_texts([text])\n",
    "#     dictionary = tokenizer.word_index\n",
    "  \n",
    "#     word2idx = {}\n",
    "#     idx2word = {}\n",
    "#     for k, v in dictionary.items():\n",
    "#         if v < VOCAB_SIZE:\n",
    "#             word2idx[k] = v\n",
    "#             idx2word[v] = k\n",
    "#         if v >= VOCAB_SIZE:\n",
    "#             continue\n",
    "          \n",
    "#     return word2idx, idx2word, tokenizer\n",
    "\n",
    "# word2idx, idx2word, tokenizer = vocab_creater(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_indices = dict((c, i) for i, c in enumerate(words))\n",
    "indices_word = dict((i, c) for i, c in enumerate(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_matrix_creater(word_indices):\n",
    "    embedding_matrix = np.zeros((len(word_indices) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_indices.items():\n",
    "        try:\n",
    "            embedding_vector = cn_vectors.get_vector(word)\n",
    "        except:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_layer_creater(VOCAB_SIZE, EMBEDDING_DIM, MAX_LEN, embedding_matrix):\n",
    "  \n",
    "    embedding_layer = Embedding(input_dim = VOCAB_SIZE, \n",
    "                                output_dim = EMBEDDING_DIM,\n",
    "                                input_length = MAX_LEN,\n",
    "                                weights = [embedding_matrix],\n",
    "                                trainable = False)\n",
    "    return embedding_layer\n",
    "\n",
    "embedding_matrix = embedding_matrix_creater(word_indices)\n",
    "embedding_layer = embedding_layer_creater(VOCAB_SIZE, EMBEDDING_DIM, SEQUENCE_LEN, embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_and_split_training_set(sentences_original, next_original, percentage_test=20):\n",
    "    print('Shuffling sentences')\n",
    "\n",
    "    tmp_sentences = []\n",
    "    tmp_next_word = []\n",
    "    for i in np.random.permutation(len(sentences_original)):\n",
    "        tmp_sentences.append(sentences_original[i])\n",
    "        tmp_next_word.append(next_original[i])\n",
    "\n",
    "    cut_index = int(len(sentences_original) * (1.-(percentage_test/100.)))\n",
    "    x_train, x_test = tmp_sentences[:cut_index], tmp_sentences[cut_index:]\n",
    "    y_train, y_test = tmp_next_word[:cut_index], tmp_next_word[cut_index:]\n",
    "\n",
    "    print(\"Size of training set = %d\" % len(x_train))\n",
    "    print(\"Size of test set = %d\" % len(y_test))\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 1\n",
    "def get_model(dropout=0.2):\n",
    "    print('Building model...')\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "#     model.add(Embedding(input_dim=len(words), output_dim=1024))\n",
    "    model.add(Bidirectional(LSTM(256)))\n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(len(words)))\n",
    "    model.add(Activation('softmax'))\n",
    "    print('Model built')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 2\n",
    "# def get_model(dropout=0.2):\n",
    "#     print('Building model...')\n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(input_dim=len(words), output_dim=1024))\n",
    "#     model.add(LSTM(512, return_sequences=True))\n",
    "#     if dropout > 0:\n",
    "#         model.add(Dropout(dropout))\n",
    "#     model.add(LSTM(512, return_sequences=False))\n",
    "#     if dropout > 0:\n",
    "#         model.add(Dropout(dropout))\n",
    "#     model.add(Dense(len(words)))\n",
    "#     model.add(Activation('softmax'))\n",
    "#     print('Model built')\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions from keras-team/keras/blob/master/examples/lstm_text_generation.py\n",
    "def sample(preds, temperature=None):\n",
    "    if temperature is None:\n",
    "        return np.argmax(preds)\n",
    "    else:\n",
    "        # helper function to sample an index from a probability array\n",
    "        preds = np.asarray(preds).astype('float64')\n",
    "        preds = np.log(preds) / temperature\n",
    "        exp_preds = np.exp(preds)\n",
    "        preds = exp_preds / np.sum(exp_preds)\n",
    "        probas = np.random.multinomial(1, preds, 1)\n",
    "        return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('./checkpoints/'):\n",
    "    os.makedirs('./checkpoints/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real code starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    examples_file.write('\\n----- Generating text after Epoch: %d\\n' % epoch)\n",
    "\n",
    "    # Randomly pick a seed sequence\n",
    "    seed_index = np.random.randint(len(sentences_test))\n",
    "    seed = (sentences_test)[seed_index]\n",
    "\n",
    "    for diversity in [None, 0.3, 0.4, 0.5, 0.6]:\n",
    "        sentence = seed\n",
    "        examples_file.write('----- Diversity:' + str(diversity) + '\\n')\n",
    "        examples_file.write('----- Generating with seed:\\n\"' + ' '.join(sentence) + '\"\\n')\n",
    "        examples_file.write(' '.join(sentence))\n",
    "\n",
    "        for i in range(100):\n",
    "            x_pred = np.zeros((1, SEQUENCE_LEN))\n",
    "            for t, word in enumerate(sentence):\n",
    "                x_pred[0, t] = word_indices[word]\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_word = indices_word[next_index]\n",
    "\n",
    "            sentence = sentence[1:]\n",
    "            sentence.append(next_word)\n",
    "\n",
    "            examples_file.write(\" \"+next_word)\n",
    "        examples_file.write('\\n')\n",
    "    examples_file.write('='*80 + '\\n')\n",
    "    examples_file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings - Data generator for fit and evaluate\n",
    "def generator(sentence_list, next_word_list, batch_size):\n",
    "    index = 0\n",
    "    while True:\n",
    "        x = np.zeros((batch_size, SEQUENCE_LEN), dtype=np.int32)\n",
    "        y = np.zeros((batch_size), dtype=np.int32)\n",
    "        for i in range(batch_size):\n",
    "            for t, w in enumerate(sentence_list[index % len(sentence_list)]):\n",
    "                x[i, t] = word_indices[w]\n",
    "            y[i] = word_indices[next_word_list[index % len(sentence_list)]]\n",
    "            index = index + 1\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings on exit - Data generator for fit and evaluate\n",
    "# def generator(sentence_list, next_word_list, batch_size):\n",
    "#     index = 0\n",
    "#     while True:\n",
    "#         x = np.zeros((batch_size, SEQUENCE_LEN), dtype=np.int32)\n",
    "#         y = np.zeros((batch_size, EMBEDDING_DIM), dtype=np.int32)\n",
    "#         for i in range(batch_size):\n",
    "# #             try:\n",
    "# #                 embedding_vector = cn_vectors.get_vector(word)\n",
    "# #             except:\n",
    "# #                 embedding_vector = None\n",
    "# #             if embedding_vector is not None:\n",
    "# #                 # words not found in embedding index will be all-zeros.\n",
    "# #                 embedding_matrix[i] = embedding_vector\n",
    "#             for t, w in enumerate(sentence_list[index % len(sentence_list)]):\n",
    "#                 x[i, t] = word_indices[w]\n",
    "#             y[i] = np.zeros(EMBEDDING_DIM)\n",
    "#             word = next_word_list[index % len(sentence_list)]\n",
    "#             try:\n",
    "#                 embedding_vector = cn_vectors.get_vector(word)\n",
    "#             except:\n",
    "#                 embedding_vector = None\n",
    "#             if embedding_vector is not None:\n",
    "# #                 # words not found in embedding index will be all-zeros.\n",
    "#                 y[i] = embedding_vector\n",
    "#             index = index + 1\n",
    "#         yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO EMBEDDINGS - Data generator for fit and evaluate\n",
    "# def generator(sentence_list, next_word_list, batch_size):\n",
    "#     index = 0\n",
    "#     while True:\n",
    "#         x = np.zeros((batch_size, SEQUENCE_LEN, len(words)), dtype=np.bool)\n",
    "#         y = np.zeros((batch_size, len(words)), dtype=np.bool)\n",
    "#         for i in range(batch_size):\n",
    "#             for t, w in enumerate(sentence_list[index % len(sentence_list)]):\n",
    "#                 x[i, t, word_indices[w]] = 1\n",
    "#             y[i, word_indices[next_word_list[index % len(sentence_list)]]] = 1\n",
    "#             index = index + 1\n",
    "#         yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_set = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored sequences: 657784\n",
      "Remaining sequences: 1323660\n"
     ]
    }
   ],
   "source": [
    "# cut the text in semi-redundant sequences of SEQUENCE_LEN words\n",
    "sentences = []\n",
    "next_words = []\n",
    "ignored = 0\n",
    "\n",
    "for i in range(0, len(text_in_words) - SEQUENCE_LEN, STEP):\n",
    "    # Only add the sequences where no word is in ignored_words\n",
    "    if len(set(text_in_words[i: i+SEQUENCE_LEN+1]).intersection(ignored_words)) == 0:\n",
    "        sentences.append(text_in_words[i: i + SEQUENCE_LEN])\n",
    "        next_words.append(text_in_words[i + SEQUENCE_LEN])\n",
    "    else:\n",
    "        ignored = ignored + 1\n",
    "\n",
    "print('Ignored sequences:', ignored)\n",
    "print('Remaining sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling sentences\n",
      "Size of training set = 1058928\n",
      "Size of test set = 264732\n"
     ]
    }
   ],
   "source": [
    "# x, y, x_test, y_test\n",
    "(sentences, next_words), (sentences_test, next_words_test) = shuffle_and_split_training_set(sentences, next_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "WARNING:tensorflow:From /Users/leonid/Projects/prj-nlp-2019/venv/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/leonid/Projects/prj-nlp-2019/venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Model built\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "checkpoint_file_path = \"./checkpoints/LSTM_GRRM-epoch{epoch:03d}-words%d-sequence%d-minfreq%d-\" \\\n",
    "            \"loss{loss:.4f}-acc{acc:.4f}-val_loss{val_loss:.4f}-val_acc{val_acc:.4f}\" % \\\n",
    "            (len(words), SEQUENCE_LEN, MIN_WORD_FREQUENCY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(checkpoint_file_path, monitor='val_acc', save_best_only=False)\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "# early_stopping = EarlyStopping(monitor='val_acc', patience=20)\n",
    "\n",
    "callbacks_list = [checkpoint, print_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"./checkpoints_word_emb/LSTM_GRRM-epoch031-words13166-sequence25-minfreq5-loss3.3405-acc0.2884-val_loss4.9648-val_acc0.1827\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def left_pad(text, req_len):\n",
    "    if len(text) == req_len:\n",
    "        return text\n",
    "    if len(text) > req_len:\n",
    "        return text[0:req_len+1]\n",
    "    text = ['.'] * (req_len - len(text)) + text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', '.', '.', '.', '.', '.', '.', '.', 'tyrion', 'drank', 'it', 'in', 'his', 'window', 'seat', ',', 'brooding', 'over', 'the', 'chaos', 'of', 'the', 'kitchens', 'below', '.']\n",
      "25\n",
      ". . . . . . . . tyrion drank it in his window seat , brooding over the chaos of the kitchens below .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leonid/Projects/prj-nlp-2019/venv/lib/python3.7/site-packages/ipykernel_launcher.py:8: RuntimeWarning: divide by zero encountered in log\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and she had taken her meals . but she could not see . she took them back , she could feel her eyes , and\n",
      "she was shaking . she could feel the emptiness in her belly , and how could she dare not feel like a child . she\n",
      "was a girl in a thousand years , but she was not to be afraid of the mother . she is still still safe to\n"
     ]
    }
   ],
   "source": [
    "diversity = 0.5\n",
    "\n",
    "sentence = 'tyrion drank it in his window seat, brooding over the chaos of the kitchens below .'\n",
    "\n",
    "sentence = clean_text(sentence).split(' ')\n",
    "sentence = left_pad(sentence, SEQUENCE_LEN)\n",
    "print(sentence)\n",
    "print(len(sentence))\n",
    "\n",
    "for i in range(100):\n",
    "    if i % SEQUENCE_LEN == 0:\n",
    "        print(' '.join(sentence))\n",
    "    x_pred = np.zeros((1, SEQUENCE_LEN))\n",
    "    for t, word in enumerate(sentence):\n",
    "        x_pred[0, t] = word_indices[word]\n",
    "\n",
    "    preds = model.predict(x_pred, verbose=0)[0]\n",
    "    next_index = sample(preds, diversity)\n",
    "    next_word = indices_word[next_index]\n",
    "\n",
    "    sentence = sentence[1:]\n",
    "    sentence.append(next_word)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = './examples/examples_1'\n",
    "examples_file = open(examples, \"w\")\n",
    "model.fit_generator(generator(sentences, next_words, BATCH_SIZE),\n",
    "                    steps_per_epoch=int(len(sentences)/BATCH_SIZE) + 1,\n",
    "                    epochs=100,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_data=generator(sentences_test, next_words_test, BATCH_SIZE),\n",
    "                    validation_steps=int(len(sentences_test)/BATCH_SIZE) + 1)\n",
    "\n",
    "\n",
    "# initial_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(\"./checkpoints/LSTM_LYRICS-epoch006-words11925-sequence10-minfreq10-loss5.3823-acc0.1647-val_loss5.4348-val_acc0.1620\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
